[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "How to talk to your bioinformatician",
    "section": "",
    "text": "1 Introduction\nPlease note: this brochure is a work in progress, barely started at this stage. The statistics chapter is largely ready, but others are at various stages. However, there still is the presentation (bihealth.github.io/howtotalk) which is actually where my idea started.\nIf you have any comments, suggestions or corrections, click in the top right corner of this page on the \\(&lt;\\) icon to open the Hypothesis annotation sidebar – it requires a free login, but is otherwise really useful and definitely worth it. Alternatively, file a github issue at the repository for this book – github.com/bihealth/howtotalk-book.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#who-this-book-is-for",
    "href": "index.html#who-this-book-is-for",
    "title": "How to talk to your bioinformatician",
    "section": "1.1 Who this book is for",
    "text": "1.1 Who this book is for\nThe book records the different bits and pieces of advice I keep giving to my collaborators, students and colleagues. They are mostly based on my experience as a statistician and computational biologist working in a biomedical research environment. A lot of what I have to say I had to learn the hard way – by commiting the mistakes I am now warning you against. I chose what things to include in this book by asking myself: “What do I wish I had known when I started out?” and: “What do I wish my collaborators knew when we started working together?”",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#who-am-i-to-tell-you-this-stuff",
    "href": "index.html#who-am-i-to-tell-you-this-stuff",
    "title": "How to talk to your bioinformatician",
    "section": "1.2 Who am I to tell you this stuff?",
    "text": "1.2 Who am I to tell you this stuff?",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#bioinformatics-statistics-and-computational-biology",
    "href": "index.html#bioinformatics-statistics-and-computational-biology",
    "title": "How to talk to your bioinformatician",
    "section": "1.3 Bioinformatics, statistics and computational biology",
    "text": "1.3 Bioinformatics, statistics and computational biology\nWe often refer to the whole general category of humans involved in biological data analysis using computers as “bioinformaticians”. However, strictly speaking, bioinformaticians in the proper sense of the word are those who develop algorithms, databases and software packages. Then, there are those who use these tools to actual data, and the more precise term for these people is “computational biologists”. Finally, there are those of us who walk around and criticize experimental designs and they are called statisticians.\nThe truth is most of us have at least set a foot in all three camps, but experience and interest usually make us specialize in one of them. In especially statistics appears to be separated from the other two, with many bioinformaticians having only a very limited understanding of it, and many statisticians having no experience with high throughput biological data and other applications which are the bread and butter of bioinformaticians and computational biologists1.\n1 A notable case in point are practical calculation of statistical power in high-dimensional settings, which require expertise in both, statistics and high-throughput data analysis.\n\n\n\n\n\n\nG\n\n\n\nstatistics\n\nStatistics\n\n\n\nhypothesis\\ntesting\n\nhypothesis\ntesting\n\n\n\nstatistics--hypothesis\\ntesting\n\n\n\n\npower analysis\n\npower analysis\n\n\n\nstatistics--power analysis\n\n\n\n\nlinear\\nmodeling\n\nlinear\nmodeling\n\n\n\nstatistics--linear\\nmodeling\n\n\n\n\nvisualizations\n\nvisualizations\n\n\n\nstatistics--visualizations\n\n\n\n\nBayesian\\nstatistics\n\nBayesian\nstatistics\n\n\n\nstatistics--Bayesian\\nstatistics\n\n\n\n\nbioinformatics\n\nBioinformatics\n\n\n\nalgorithms\n\nalgorithms\n\n\n\nbioinformatics--algorithms\n\n\n\n\nformalizations\n\nformalizations\n\n\n\nbioinformatics--formalizations\n\n\n\n\nstandards\n\nstandards\n\n\n\nbioinformatics--standards\n\n\n\n\ndatabases\n\ndatabases\n\n\n\nbioinformatics--databases\n\n\n\n\ncbio\n\nComputational\nbiology\n\n\n\ndata\\nscience\n\ndata\nscience\n\n\n\ncbio--data\\nscience\n\n\n\n\nfunctional\\ngenomics\n\nfunctional\ngenomics\n\n\n\ncbio--functional\\ngenomics\n\n\n\n\nusing tools\n\nusing tools\n\n\n\ncbio--using tools\n\n\n\n\nsequence\\nanalysis\n\nsequence\nanalysis\n\n\n\ncbio--sequence\\nanalysis\n\n\n\n\nComputers and science\n\nComputers\nand\nScience\n\n\n\nComputers and science--statistics\n\n\n\n\nComputers and science--bioinformatics\n\n\n\n\nComputers and science--cbio\n\n\n\n\nimplementation\n\nimplementation\n\n\n\nalgorithms--implementation\n\n\n\n\nmaintenance\n\nmaintenance\n\n\n\ndatabases--maintenance\n\n\n\n\ndata\\nstewardship\n\ndata\nstewardship\n\n\n\ndata\\nscience--data\\nstewardship\n\n\n\n\ngene set\\nenrichment\n\ngene set\nenrichment\n\n\n\nfunctional\\ngenomics--gene set\\nenrichment\n\n\n\n\nphylogenies\n\nphylogenies\n\n\n\nsequence\\nanalysis--phylogenies\n\n\n\n\np-values\n\np-values\n\n\n\nhypothesis\\ntesting--p-values\n\n\n\n\neffect sizes\n\neffect sizes\n\n\n\nhypothesis\\ntesting--effect sizes\n\n\n\n\nhierarchical\\nmodeling\n\nhierarchical\nmodeling\n\n\n\nlinear\\nmodeling--hierarchical\\nmodeling",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#what-bioinformaticians-like",
    "href": "index.html#what-bioinformaticians-like",
    "title": "How to talk to your bioinformatician",
    "section": "1.4 What bioinformaticians like",
    "text": "1.4 What bioinformaticians like",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "statistics.html",
    "href": "statistics.html",
    "title": "2  Statistics",
    "section": "",
    "text": "2.1 We are all Bayesians\nWhen talking about statistics to students, I like to show a simple coin trick. It isn’t really a trick at all, but for me realizing what it shows for the first time completely changed the way how I understand the whole field.\nImagine that I put a coin on a table, heads up. The first question I ask you is: what is the probability that the coin is heads up? What is the probability that the coin is tails up? You can clearly see that the coin is heads up, so you tell me that the first probability - heads up - is 100%, and the probability that it is tails up is 0%.\nI take the coin in my hand and as I am about to throw it in the air I ask you what is the probability that it lands heads up. Usually you will say 50%. That is an obvious answer. But now, I actually throw the coin, catch it and land it on my hand - but I still cover it with my hand, so none of us can see how the coin landed. I ask you again: what is the probability that the coin shows heads? Now you might start to get suspicious, but the answer is still pretty much obvious: 50%, 1:1.\nOK, but now the plot thickens. I raise my hand just a bit and take a peek on the coin. You still can’t see it, but I exclaim: “AHA!” - I know already how it landed. I ask my question again: what is the probability that it landed heads up? Maybe you tell me: it is still 50%. But wait, I have already seen whether it is heads up, right? For me the situation is the same as at the very beginning, when the coin was still on the table. So for me, it is either 0% - if it landed tails up - or 100%, if it showing heads?\nBut how can probability be relative? Can it be relative at all?\nThe answer is, maybe surprisingly: it can’t. At least not in the “regular”, frequentist statistics - the one that we use in most of our scientific papers. In fact, the very moment that the coin landed, the probability was 0 or 100%, whether we were aware of it or not.\nThat is why, in classical statistics, the p-values are not really to tell us any probability about our particular statistical test! If we were to compare two groups with a t-test, then either there are differences between the groups or there aren’t any - the coin has already landed, the probability is either 0% or 100%. What the p-values are meant to do is to keep is from making a mistake (specifically, a type I error) in the long run. However, whether we made a mistake or not is, in a particular case, 0% or 100%, we just don’t know which one. However, we can be reasonably confident that the percentage of such mistakes in our research will be less than 5% (or whichever p-value threshold we choose).\nOur intuition was therefore wrong… but was it, really?\nIt turns out – it was wrong, but only in the classic, frequentist statistics. However it was dead on right from the point of view of Bayesian statistics. In Bayesian statistics, probability is a measure of information. We start with a rough idea based on some prior knowledge - that the coins are generally fair and land 50% of the time showing heads. We do an experiment, and we update this knowledge by observing that it landed actually showing heads, and we increase the probability to 100%. Until we make the observation, we keep the prior - whether the coin has already landed or not, as long we don’t know how it landed, the probability is still 50%. Thus, our intuition is definitely bayesian.\nThis makes for an interesting conundrum: we normally use frequentist statistics, but we think in bayesian terms. This has profound and unfortunate consequences, because p-values are a part of the language of science. We use p-values and statistical hypothesis testing on a regular basis to communicate with other researchers, to tell them something about our results, but this language is unintuitive and poorly understood by many.\nIs the Bayesian statistics “more correct” than the frequentist approach? There is no easy answer to this question. The debates between frequentist statisticians and bayesians can actually get quite heated at times. However, whatever our views on frequentists statistics are, the majority of the scientific papers will still involve p-values of some sort of another, and that means that you need to understand what p-values are.\nTherefore, let us talk about what p-values are, what their drawbacks are and what we can do about it.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "statistics.html#we-are-all-bayesians",
    "href": "statistics.html#we-are-all-bayesians",
    "title": "2  Statistics",
    "section": "",
    "text": "One of the reasons that the “frequentist” hypothesis testing is so firmly established is that bayesian statistics requires way more computing resources, which only relatively recently became widely available to statisticians.\n\n\n\n\n\n\n\n\nFrequentist Statistics\nBayesian Statistics\n\n\n\n\n\n\n\n\n1. Probability is defined as the long-run frequency of events\n1. Probability represents a degree of belief or certainty about an event\n\n\n2. Parameters (like the “true value”) are fixed but unknown quantities.\n2. Parameters are treated as random variables with their own probability distributions.\n\n\n3. Asking about the probability of a hypothesis does not make sense\n3. Asking about the probability of a hypothesis is the main goal",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "statistics.html#how-to-understand-p-values",
    "href": "statistics.html#how-to-understand-p-values",
    "title": "2  Statistics",
    "section": "2.2 How to understand p-values",
    "text": "2.2 How to understand p-values\n\n2.2.1 What is a p-value?\nSay you want to run a statistical test - such as a t test - to decide whether two groups differ. You want to distinguish between two possibilities:\n\nThere are no differences between the groups\nThere are differences between the groups\n\nWe call the former situation \\(H_0\\) or the null hypothesis, and the latter (there are differences), the \\(H_1\\) or the alternative hypothesis.\nSay you run the test, and you got a p-value of 0.02. What does that mean? P-value is a probability (this is where the p in “p-value” comes from), but it is the probability of what, exactly?\nTake a moment and try to answer this question. Chances are, your answer is one of these:\n\nProbability that \\(H_0\\) is true (probability that there is no difference), given the data\nProbability that \\(H_1\\) is true (probability that there is a difference), given the data\nProbability that the data is random\nProbability that the observations are due to random chance\nProbability of getting the same data by random chance\n\nIf your answer is above, I have good and bad news. The bad news is that you are wrong, because the correct answer is not among the answers above. The good news is that you are in a good company, research shows that something like half of the scientists using p-values don’t really understand them. Oh wait, that means that a lot of scientists don’t understand p-values, that can’t be good - I guess I have only bad news for you, then.\nFor the purposes of this discussion, we can simplify a formal definition to the following:\n\n\n\n\n\n\nP-value is the probability of observing an effect at least as extreme (as the one we observed) given that \\(H_0\\) is true.\n\n\n\nIn other words, assuming that there are no differences, but keeping all our other assumptions about the data (for example, how it is distributed), we ask what is the probability of getting data that show the same or larger differences between the groups?\nLet us dive in it a bit deeper.\nWhen you run a t-test, you calculate the so-called t statistic. It is the measure of the difference between the groups that we observe. It takes into account the differences and how many data points we have. What we do in the t-test we are assuming that there are no differences between the group, and ask, how likely it is to get a t value equal or larger to the one that we got under this assumption.\n\n\n2.2.2 Statistical tests make assumptions\nAs you can see, this definition of p-value I have included here is closest to the last of the wrong answers I have written out before – “probability of getting the same data by random chance”. However, there is a subtle but extremely important difference between the formulations, and I don’t mean that one uses “same data” and the other uses “an effect at least as extreme”. What I am talking about is that we are not looking at random data. The effects that we calculate under \\(H_0\\) must fullfill a lot of assumptions of the statistical test.\nYou probably heard of one of this assumptions - the assumption of normality. Parametric tests such as the t-test assume that the data is (roughly) normally distributed. If your data is not normally distributed – and a lot of biological data is not normally distributed – then this assumption is violated, and the p-value is calculated based on a model that does not fit our data.\nHowever, we could successfully argue that, in fact, no real data is normally distributed. Consider such a trait as the height of a human being. This is one of the traits that are par excellence the examples of normally distributed data. However, an actually normal variable can take any value with a certain, non-zero probability. In other words, there should be a non-zero probability of having a human being with height, say, 10 kilometers, or maybe have a negative length of -20 cm. Which, of course, doesn’t make any sense.\n\n\nNormality testing. You can test the data for normality. However, you need to be cautious. First, without enough samples, you will never be able to reject the assumption of normality. And if you have a lot of samples, you will almost always reject the assumption of normality. In other words, whether you reject the assumption of normality or not depends to some extent on your number of samples. Some statisticians, myself included, think that testing for normality of the data is therefore a waste of time.\nHowever, we know that many parametric tests – such as t-test and ANOVA – ara actually fairly robust when it comes to the violation of normality. In other words, you can safely use them with data which is not really normally distributed. This is one of these things that make statisticians remember the famous quote of George E. P. Box: “All models are wrong, but some are useful”.\nHowever, there are also other assumptions, of which one of the most important ones is the assumption of independence.\n\n\n2.2.3 Assumption of independence\nThis actually happened: I have been comparing the transcriptomes of two groups of people, healthy controls and patients suffering from a disease. What I saw was a large number of genes related to sex chromosomes. When I got the full meta-data of the project, it turned out that while the patients were 90% male, almost all of the healthy controls were health care workers, primarily nurses – in other words, females.\nSuch a setup violates the assumption of independence: that each measurement is independent of other measurements. This makes any results of statistical hypothesis testing in this setup unreliable, whatever test you are using, whether it is parametric or not etc.\n\n\n\n\n\n\nAssumption of independence is really, really important.\n\n\n\nOne way of dealing with that is to use the sex of the individual as another coefficient in the model. This has the drawback of making the model more complex and decreasing your statistical power, but is otherwise a good way to proceed. Unless, of course, all the males are in one group, and all the females are in the other – in such a case there is no good way to study the data. You just wasted a lot of time and money.\nAnother way that the assumption of independence is typically violated is a batch effect. Some technologies are more prone to batch effects than others, and you should always attempt to preemptively plan your experiments in a way that minimizes potential batch effects. One such technique is randomization – you randomly assign your samples to batches, so batches and groups do not correlate.\nWhile batch effects can be removed to a certain degree – and there are specialized tools for that – this never works to a 100%, and even when it works, it will not help you with a botched experiment design, in which groups and batches coincide.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "statistics.html#problems-with-p-values",
    "href": "statistics.html#problems-with-p-values",
    "title": "2  Statistics",
    "section": "2.3 Problems with p-values",
    "text": "2.3 Problems with p-values\nP-values have all sorts of problems. Some of them are inherent and the result of p-value properties. Others are due to how we handle them.\n\n\nThe ASA statement on p-values: context, process, and purpose (Wasserstein and Lazar 2016)\n\n2.3.1 P-values have limited informative value\nWhat would you say if I told you that p-values are not informative about your specific experiment that you are considering? As we discussed, p-values are calculated assuming that there is no effect – for example, no difference between the groups. However, when we find that \\(p &lt; 0.05\\), we reject the \\(H_0\\). The p-value is then calculated based on an assumption that was rejected, that we decided is not real. In our world (in which there is a difference between the groups), the p-value does not describe any reality. It belongs to an alternate world, in which there was no difference between the groups.\nHowever, what p-values definitely do in a reasonable way is to control the false discovery rate. While it can be argued (as above) that p-values are not informative for the particular experiment we are considering, using p-values protects you from too many false positives in the long run. It works as follows: even assuming no effect (no differences between the groups), there is a certain probability of observing large differences once in a while. By carefully considering this possibility with p-values we are attempting to make sure that this rare events do not fool us more than once in 20 times (which corresponds to the p-value threshold of 0.05).\n\n\n\n\n\n\nP-values control they false positive rate (type I error rate). This is all they do.\n\n\n\nHowever, p-values are not XXX\n\n\n2.3.2 Absence of evidence is not evidence of absence\nThe very name “type I error” suggests that there is another, second type of error. Indeed there is: the type II error. While the type I error is getting a false positive – concluding a difference where there isn’t one – the other side of the coin is doing the opposite – concluding a lack of difference when there is, in fact, a difference. This is called a false negative.\nThe bad news is, p-values do absolutely nothing to help with controling the false negative rate. The even worse news is that these errors are really, really common. You have probably heard of “statistical power”: that is exactly the ability of a statistical test to avoid false negative errors. It turns out that even for most powerful tests in best possible settings this power rarely exceeds 80%: in other words, 20% of true differences are never discovered!\n\n\n\n\n\n\nLack of statistical significance does not “prove” that there is no difference. Instead of saying “there was no difference” say “we failed to detect a statistically significant difference”.\n\n\n\nThis has several practical consequences. One of the most important is that the fact that the difference between the groups was not statistically significant does not mean that there is no difference.\n\n\n2.3.3 Difference between statistically significant and non-significant is not significant\nAndrew Gelman coined the phrase “the difference between statistically significant and non-significant is not, itself, statistically significant”. What it means is this: if in one comparison you get a statistically significant difference, and in the other one you see that there is no significant difference, then you cannot conclude that the there is a difference between comparisons.\nOh my, I think I have utterly confused you now. I think, however, that the problem is important and worth looking into, because while it is a surprisingly common way of reasoning, it is utterly wrong and can lead you astray.\nConsider this. You have two strains of mice, a wild type (WT) and a knockout. Your hypothesis is that while in the WT the regulation of a cytokine production works correctly and the cytokine gets produced upon stimulation, the regulation is broken in the KO. You plan your experiment as follows: for each of the mouse strains, you have a control (Ctrl) and a treatment (Trt) group, and your response variable is the level of the cytokine you have measured.\nNow to evaluate your experiment, you compare Ctrl with Trt in the WT strain and also in the KO strain. You have two comparisons: in WT (\\(\\Delta\\)WT = TrtWT - CtrlWT) and in KO (\\(\\Delta\\)KO = TrtKO - CtrlKO). In both cases, you run a t-test. You find that the p-value from the first comparison (\\(\\Delta\\)WT) is significant (\\(p &lt; 0.05\\)), and the p-value from the second comparison (\\(\\Delta\\)KO) is not (\\(p &gt; 0.05\\)). Does that mean that you confirmed your hypothesis? Can you say that there is no difference in KO, but there is a difference in WT?\nThe short answer is: no, you can’t.\nYou were able to show that there is a significant difference in the WT strain, but you failed to show that there is a difference in the KO. And failing to show a difference is not the same as showing a lack of difference. This is erronous reasoning is widely spread – a while ago a team of researchers looked at bunch of publications and found that almost half of the scientists that could make this error, did it.\n\n\nErroneous analyses of interactions in neuroscience: a problem of significance (Nieuwenhuis, Forstmann, and Wagenmakers 2011).\nSo what should we do instead? In fact we have to run a single statistical test. We have the differences in KO (\\(\\Delta\\)KO), and the differences in the WT (\\(\\Delta\\)WT), and we need to test whether these differences differ, whether \\(\\Delta_{KO} \\neq \\Delta_{WT}\\). This difference of differences is called, in statistics, interaction. To put it in a slightly more formal way, you have a 2-factor experiment (with one factor being the mouse strain, KO or WT, and the other being the treatment or lack thereof) and what you need to test is the interaction between the two factors.\nThe bad news (I seem to be the bringer of bad news all throughout this chapter) is that this test for interaction is much weaker (in terms of statistical power) than a simple comparison between two groups. That means that in order to have the same chance to detect a real effect as in the case of a simple comparison, you have to use many more samples.\nBut wait, there’s more. More bad news to come.\n\n\n2.3.4 Spurious results can be very convincing\nOne of the most common applications of high throughput sequencing is RNA-Seq, in which we look at transcriptomes (so, expression of thousands of genes at once) in some samples. Quite frequently, one needs to use a two factor design, for example comparing control and treatment groups in either KO or WT strains. The correct way of analysing such an experiment is, as mentioned, testing for a significant interaction. However, quite frequently the analysis is done differently.\n\n\nVenn diagrams may indicate erroneous statistical reasoning in transcriptomics (Weiner 3rd, Obermayer, and Beule 2022).\nI have followed that different approach in a paper I wrote a few years ago. I used gene expression data from both COVID-19 patients and non-COVID-19 patients. In each of these two groups (C19 and non-C19) there were two groups of patients, which I will call for now G1 and G2 (I will tell you what they are in a few moments). The question was: is the difference between COVID-19 patients and non-COVID-19 patients influenced by whether the patients belong to G1 or G2? In other words, does it matter for COVID-19 if you are a G1 or a G2 patient? For example, is COVID-19 more severe in G1? Are immune responses different for G1 patients than for G2 patients?\n\nWhat you see on the top left on the figure above is a Venn diagram. It shows the number of genes which are significantly different between COVID-19 and non-COVID-19 patients in G1 and in G2, respectively. We can see that 431 genes are specifically different in G1 only, and 278 are specifically different in G2 only, while only 132 are common for the two groups. When you run gene set enrichment on these G1- or G2-specific genes, you will find different facets of the immune responses, like TLR-4 signalling in G1 and IL-7 mediated signalling in G2. It would be really easy to write a paper on how the groups G1 and G2 shape the immune responses to COVID-19. So not only we see that G1 and G2 differ, but we also see that these differences make biological sense.\nThe problem is that G1 and G2 are random samples from the same population. I have pulled these samples from one large group by random! There are absolutely no real differences between G1 and G2. And yet it was easy for me to find apperent differences, show them on a nice plot and even attribute certain specific biological processes to them. How did I do it?\nStart with the “specific” genes. What does it mean that a gene is in the group of 431 genes “specific” for G1? It means that it was significant in the comparison between COVID-19 and non-COVID-19 in G1, but was not significant in G2. But as I mentioned before, the fact that it was significant does not mean that it was not different in G2 or that it is specific for G1. Perhaps the p-value was 0.049 for G1 and 0.051 in G2, and the actual differences in G1 and G2 were similar.\nBut if all that we are seeing on the Venn diagram is white noise, how come we have these beautiful and meaningful gene set enrichment terms? The point is that not only genes not significant in G2 may, in fact, also differ between COVID-19 and non-COVID-19 patients, albeit we fail to detact this fact; they are also very likely to be also genes which are involved in the immune responses. If a gene is related to the IL-7 mediated signalling and is significantly different in G2, it will increase the chance of detecting this pathway in G2. However, the very same gene is likely to also be regulated in G1, but our failure to detect it - because of lack of statistical power or because of random noise – means that we can’t identify the IL-7 pathway in G1. That does not, however, mean that it is not there.\nThe correct analysis in this case is, as mentioned before, analysis of interactions. If done correctly, the analysis does not show any differences between G1 and G2.\n\n\n2.3.5 Statistically significant is not the same as significant\n\n\n2.3.6 We treat p-values with too much reverence\nchasing of 0.05, p-value hacking and cherry picking",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "htdata.html",
    "href": "htdata.html",
    "title": "3  High Throughput Data Analysis",
    "section": "",
    "text": "3.1 Arnolfini and Venice\nIn the previous chapter, we touched on analysis of high throughput (HT) data. Here, we will spare some more time to discuss some of the strong and weak points of HT data.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>High Throughput Data Analysis</span>"
    ]
  },
  {
    "objectID": "htdata.html#arnolfini-and-venice",
    "href": "htdata.html#arnolfini-and-venice",
    "title": "3  High Throughput Data Analysis",
    "section": "",
    "text": "There are more connections of HT data to impressionism. A famous software package for HT data analysis is called Seurat – from the name ofa famous French painter, Georges Seurat. Seurat painted using the so called pointillism technique, where small dots of color are applied in patterns to form an image. The corresponding analysis package is used, among others, to visualize single cell RNA-seq data, where each of the thousands of dots represents a single cell.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>High Throughput Data Analysis</span>"
    ]
  },
  {
    "objectID": "htdata.html#exploratory-vs-hypothesis-driven-research",
    "href": "htdata.html#exploratory-vs-hypothesis-driven-research",
    "title": "3  High Throughput Data Analysis",
    "section": "3.2 Exploratory vs Hypothesis-driven research",
    "text": "3.2 Exploratory vs Hypothesis-driven research",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>High Throughput Data Analysis</span>"
    ]
  },
  {
    "objectID": "htdata.html#statistics-and-ht",
    "href": "htdata.html#statistics-and-ht",
    "title": "3  High Throughput Data Analysis",
    "section": "3.3 Statistics and HT",
    "text": "3.3 Statistics and HT\n\n3.3.1 Correction for multiple testing / FDR\nIf you are reading this book, you probably have heard about adjusted p values or FDRs (false discovery rates). What are they and why do we need them?\nOver a decade ago, a group of scientists bought a dead fish on a fish market and brought it back to the lab, where they put it into a functional MRI (fMRI) machine. Here is what they did:\n\n\nNeural correlates of interspecies perspective taking in the post-mortem Atlantic salmon: An argument for proper multiple comparisons correction. (Craig M. Bennett 2011)\n\nThe task administered to the salmon involved completing an open-ended mentalizing task. The salmon was shown a series of photographs depicting human individuals in social situations with a specified emotional valence. The salmon was asked to determine what emotion the individual in the photo must have been experiencing.\n\nYes, you read this correctly. They put a dead fish into an fMRI and then asked it to look at photographs and to think. And then they measured the brain activity of the salmon and, quite surprisingly, they did find some apparent activity in the brain of the fish.\nThe problem was, you see, that at that time standard evaluation of fMRI involved performing a diabolical number of statistical tests, comparing voxels (3D pixels) between different conditions. And the standard way of determining which of them are significant was to use a simple (and lax) p-value threshold. The authors of the salmon study argued that this is not admissible, because you will always find some “significant” results, and argued for the use of multiple correction procedures1.\n1 The authors of the salmon poster gained a lot of deserved attention with their stunt, culminating with an IgNobel prize in neuroscience in 2012. You can still find their poster online.The problem of multiple testing is present throughout all of the high throughput data analysis techniques. Each time we do a statistical test with a predefined significance level (the notorious “\\(p &lt; 0.05\\)”), we are trying to limit the occurence of false positive results (type I errors). Using p-values and a threshold of 0.05 means that we are willing to accept, on average, 1 false positive result in 20 tests. However, if we run just one RNA-seq experiment with one comparison, we will be doing something like 10 to 20 thousands of statistical tests. In other words, at \\(p &lt; 0.05\\), we expect 500 to 1000 false positive results, even if there is no real difference between the conditions!\nThat is why we need to employ a procedure to control the false positive rate2. The widely used Benjamini-Hochberg procedure3 aims at controlling the false discovery rate (FDR). The principle is as follows: among all genes for which the FDR is below 0.05, we expect at most 5% false positives. Similarly, if we only consider genes with FDR below 0.01, only 1% of them are expected to be false positives. FDR is therefore not, strictly speaking, a p-value (even though we often call it “adjusted p-value” and use it as such).\n2 Actually, there are two general approaches: correcting the family wise error rate (FWER), which is the probability of making at least one type I error in a family of tests, and controlling the false discovery rate (FDR), which is the expected proportion of type I errors among the rejected hypotheses. Benjamini-Hochberg procedure is the latter, the Bonferroni correction is the former.3 Described in one of the most highly cited papers ever, with an order of magnitude more citations than the Watson and Crick paper on the structure of DNA.However, the side effect of multiple testing correction is that it severly reduces the statistical power of the analysis. In other words, many genes that are actually differentially expressed will have an FDR above the threshold of 0.05, and we will not be able to detect them. Consider this: if you have 500 genes at FDR &lt; 0.05, you expect that 25 of them are false positives - and therefore 475 are true positives, really differentially expressed. However, if at the same time you have 1500 genes at FDR &lt; 0.1, it means that within these 1500 genes there are 150 false positives (additional 125 to the 25 already counted at FDR &lt; 0.05), but also 1350 true positives - an additional 875 true positives with FDR between 0.05 and 0.1. These are true positives that we will miss at FDR &lt; 0.05!\nThe bottom line is that we control the type I error rate – the false positives – at the cost of increasing the type II error rate – the false negatives. This is a dirty compromise that we encounter in many places in statistics and machine learning4, but it is exacerbated in the context of HT data analysis.\n4 And even medicine: if you raise the specificity of a test (minimizing false positives) you will lower its sensitivity (increasing false negatives) and vice versa. Screening tests typically aim at a high sensitivity (to catch all potential patients), but the cost is a low specificity – and that is why you should not worry too much about a positive result of mammography or PSA test without further confirmation.\n\n\n\n\n\nMultiple testing correction is essential when analyzing HT data, but it severly reduces the statistical power of the analysis.\n\n\n\n\n\n3.3.2 The scourge of the absolute thresholds\nA question that often arises in the context of HT data analysis is: “so how many genes / proteins / metabolites are differentially expressed exactly?”. For me as a bioinformatician, this question is not easily answered, and I will try to explain why. In the following, I will talk mostly about (bulk) RNA-seq data, but what I say applies to all kinds of HT data as well. In this context, we often say “DEGs” (differentially expressed genes) to refer to genes that are expressed at different levels in two or more conditions.\nBut what is a DEG? How do you know a gene is a DEG? Quite often, we define the DEGs by the results of a statistical comparison we have made between two groups of samples. We define DEGs as those genes for which the FDR (false discovery rate) is below a certain threshold, e.g. 5% (\\(FDR &lt; 0.05\\)). Why 5%? This comes from the common practice in statistics, but that does not make it any less arbitrary. We could use 1% or 10%, or even 2.7%. And depending how we choose the threshold, the number of DEGs will be different.\nBut 5% has at least a certain meaning: it is an attempt to keep the fraction of false positives among the DEGs at a certain level, as explained in the previous section (“Correction for multiple testing”). However, we also add another arbitrary threshold: the absolute change of the log fold change. The problem is that the statistical power of the test to compare the results depends on the absolute expression level of a given gene. Simplifying the matter a bit, genes with high expression can be measured more confidently, so we can detect smaller changes in their expression. However, small changes in expression are often biologically not very interesting. Technically, we want to filter the DEGs not only based on statistical significance, but also on the effect size, i.e. the absolute change in expression.\nQuite often, we use a threshold of 2-fold change, at least a two-fold increase or two-fold decrease (which corresponds to an absolute log2 fold change of 1 or more). Again, this is completely arbitrary5, and many people use different thresholds, or none at all.\n5 Even more than the FDR, which is rooted in a hundred years of tolerating one mistake in 20.And of course, the actual number of DEGs will depend on the thresholds you have chosen. This is why, when asked to provide the number of DEGs, I sometimes ask back: “how many would you like me to find?”.\nHowever, given a particular definition in a paper or experiment, we could argue that the number of DEGs is well defined. This is true, but the problem starts when you actually try to pretend that the number of DEGs is informative and useful. Because my next question to you would be: what do you need the number of DEGs for? The truth is, it is often used as a proxy for the overall “amount of change”. For example, you might try to compare the number of DEGs between different experiments. This is how it quickly is reported in papers: “we found 1234 DEGs in treatment A compared to controls, but only 567 in treatment B, demonstrating that treatment A has a stronger effect on gene expression than treatment B”.\nUnfortunately, this may be completely misleading. First of all, the number of DEGs will strongly depend on the statistical power of the experiment. If in one comparison we are looking at a group of 10 samples vs 10 samples, and in the other we have only 3 vs 3 samples, even if the extent of regulation is the same in both comparisons, there will be fewer DEGs in the second comparison, simply because we have less statistical power to detect them.\nBut even if we keep the sample size the same, the number of DEGs will depend on the overall variability of the data, on small errors in the preparation of the samples, on outliers etc etc. Yes, it is true that if the overall extent of regulation is higher, the number of DEGs will likely to be higher as well, but the reverse doesn’t necessarily hold. In the following section, we will look at an example and alternative ways of getting an idea of the overall extent of regulation.\n\n\n\n\n\n\nAvoid using absolute thresholds if possible. They are much less meaningful that they seam. Use meaningful visualizations instead, e.g. volcano plots, MA plots or disco plots.\n\n\n\n\n\n3.3.3 DEGs and p-value trap: an example\nIn the code below, I will use data which can be found in the GEO database under accession number GSE156063. The expression data comes from patients who either suffered from COVID-19 or another respiratory disease (it is the same data set that I used to demonstrate the incorrect use of Venn diagrams in a previous chapter). The code to generate the results and figures below is included in this book for your pleasure.\n\n\nCode\nlibrary(DESeq2)\n# the results are loaded from the output of DESeq2-based analysis of the\n# data set using a pipeline called sea-snap (see\n# github.com/bihealth/seasnap-pipeline) and loaded with the R package\n# Rseasnap.\n# However, we will only use the raw counts and the covariate table here.\nlibrary(Rseasnap)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(cowplot)\n\n\n\n\nCode\npip &lt;- load_de_pipeline(\"DE_config.yaml\")\n\ncovar &lt;- get_covariates(pip)\ncnts &lt;- counts(get_deseq2(pip))\n\n\nFirst, a little demonstration. We will run the same comparison, between non-COVID viral infection and non-viral controls, twice: once with 5 randomly selected samples per group, and once with 5 additional samples per group, and we will compare the results.\n\n\nCode\n# let us write a little function so that we don't have to repeat the same\n# code twice. The function runs DESeq2 on a selected subset of samples\nrun_deseq2 &lt;- function(covar, cnts, sel,\n                       contrast = c(\"group\", \"other\", \"no\")) {\n  covar_sel &lt;- covar[sel, ]\n  cnts_sel &lt;- cnts[, sel]\n  ds &lt;- DESeqDataSetFromMatrix(\n    countData = cnts_sel,\n    colData = covar_sel,\n    design = ~ group\n  )\n  ds &lt;- DESeq(ds)\n  res &lt;- results(ds, contrast = contrast)\n  as.data.frame(res)\n}\n\ncovid &lt;- which(covar$group == \"SC2\")\nno_covid &lt;- which(covar$group == \"no\")\nother_resp &lt;- which(covar$group == \"other\")\n\n# set the random number seed so you can reproduce the results\nset.seed(12345) \n\nsel_5 &lt;- c(sample(no_covid, 5), sample(other_resp, 5))\nres_5 &lt;- run_deseq2(covar, cnts, sel_5)\n\n# add 5 more samples per group\nsel_10 &lt;- c(sel_5, sample(setdiff(no_covid, sel_5), 5), \n                   sample(setdiff(other_resp, sel_5), 5))\nres_10 &lt;- run_deseq2(covar, cnts, sel_10)\n\nsel_10_covid &lt;- c(sample(covid, 10), sample(other_resp, 10)) \nres_10_covid &lt;- run_deseq2(covar, cnts, sel_10_covid,\n                           contrast = c(\"group\", \"other\", \"SC2\"))\n\nres_m &lt;- merge(\n  res_5 %&gt;% \n    select(log2FC_5 = log2FoldChange, pval_5 = pvalue, padj_5 = padj) %&gt;% rownames_to_column(\"gene\"),\n  res_10 %&gt;% \n    select(log2FC_10 = log2FoldChange, pval_10 = pvalue, padj_10 = padj) %&gt;% rownames_to_column(\"gene\"),\n  by = \"gene\") |&gt; merge(\n  res_10_covid %&gt;% \n    select(log2FC_10_covid = log2FoldChange, pval_10_covid = pvalue, padj_10_covid = padj) %&gt;% rownames_to_column(\"gene\"),\n  by = \"gene\"\n)\n\n\nAt FDR &lt; 0.05 and with a 2-fold change threshold (\\(abs(log2FC) &gt; 1\\)), we find 1285 DEGs with 5 samples per group, but 2372 DEGs with 10 samples per group. This is quite a difference! I have seen papers in which much finer differences in the number of DEGs were used to argue that one condition has a stronger effect on gene expression than another. But are the results really that different? Let us take a look at the volcano plots, which show the log fold change on the x-axis and the negative log10 of the p-value on the y-axis6.\n6 The negative log10 of the p-value is used so that small p-values (which we are interested in) are more clearly distninguished.\n\nCode\ndf &lt;- rbind( res_5 |&gt; rownames_to_column(\"gene\") |&gt; mutate(set = \"5 samples\n            / group\"), res_10 |&gt; rownames_to_column(\"gene\") |&gt; mutate(set =\n            \"10 samples / group\")) |&gt; mutate(significant = ifelse(padj &lt;\n0.05 & abs(log2FoldChange) &gt; 1, \"yes\", \"no\"))\n\nggplot(df, aes(x = log2FoldChange, y = -log10(padj))) +\n  geom_point(aes(color = significant), alpha = 0.1) +\n  scale_color_manual(values = c(\"no\" = \"black\", \"yes\" = \"#880000\")) +\n  geom_hline(yintercept = -log10(0.05), linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = c(-1, 1), linetype = \"dashed\", color = \"blue\") +\n  facet_wrap(~set) +\n  xlab(\"log2 fold change\") +\n  ylab(\"-log10 adjusted p-value\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 3.1: Volcano plots for the comparison of non-COVID viral infection vs non-viral controls with 5 samples per group (left) and 10 samples per group (right).”\n\n\n\n\n\nAs you can see, there is hardly a big difference. Despite the difference in the absolute number of DEGs, the actual volcano plots are quite similar. We can further show that on a plot directly comparing the log2 fold changes. In addition, we will color the genes with a score that combines p-values and log2 fold changes, so called “disco” (discordance / concordance) score. Genes that show similar behavior in both comparisons come up red, while genes that go into opposite directions are blue.\n\n\nConcordant and discordant gene expression patterns in mouse strains identify best-fit animal model for human tuberculosis (Domaszewska et al. 2017)\nIn addition, to better appreciate the similarity of the results for 5 and 10 samples, on the other panel of the same figure I will throw another comparison into the mix: again with 10 samples per group, but this time COVID-19 patients vs non-viral controls.\n\n\nCode\n#|\n\n# cutoff threshold for disco score\ndthr &lt;- 20\n\n# calculate disco scores\nres_m &lt;- res_m |&gt;\n  mutate(disco_5_10 = log2FC_5 * log2FC_10 * abs(log10(pval_5) + log10(pval_10)),\n         disco_5_10_covid = log2FC_5 * log2FC_10_covid * abs(log10(pval_5) + log10(pval_10_covid))) |&gt;\n  mutate(disco_5_10 = ifelse(abs(disco_5_10) &gt; dthr, sign(disco_5_10) * dthr, disco_5_10),\n         disco_5_10_covid = ifelse(abs(disco_5_10_covid) &gt; dthr, sign(disco_5_10_covid) * dthr, disco_5_10_covid))\n\np1 &lt;- ggplot(res_m, aes(x = log2FC_5, y = log2FC_10, color = disco_5_10)) +\n  geom_point(alpha = 0.3) +\n  scale_color_gradient2(low = \"blue\", mid = \"grey\", high = \"red\", midpoint = 0) +\n  xlab(\"log2 fold change (5 samples / group)\") +\n  ylab(\"log2 fold change (10 samples / group)\") +\n  theme_minimal() +\n  ggtitle(\"5 samples vs 10 samples\") +\n  theme(legend.position = \"none\") +\n  xlim(c(-6, 6)) + ylim(c(-6, 6)) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"black\")\n\np2 &lt;- ggplot(res_m, aes(x = log2FC_10, y = log2FC_10_covid, color = disco_5_10_covid)) +\n  geom_point(alpha = 0.3) +\n  scale_color_gradient2(low = \"blue\", mid = \"grey\", high = \"red\", midpoint = 0) +\n  xlab(\"log2 fold change (10 samples / group, non-COVID vs no)\") +\n  ylab(\"log2 fold change (10 samples / group, COVID vs no)\") +\n  theme_minimal() +\n  ggtitle(\"COVID19 patients or non-COVID19 patients compared with no virus controls\") +\n  theme(legend.position = \"none\") +\n  xlim(c(-6, 6)) + ylim(c(-6, 6)) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"black\")\n\nplot_grid(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\nFigure 3.2: Disco plots comparing log2 fold changes. Left: comparison of 5 samples per group vs 10 samples per group for non-COVID viral infection vs non-viral controls. Right: comparison of 10 samples per group for COVID viral infection vs non-viral controls.\n\n\n\n\n\nAs you can see, the results obtained for 5 and 10 samples per group are basically identical (Pearson’s \\(r\\) is 0.88). On the other hand, it is clear that the comparison of COVID-19 with non-viral disease, while similar, yields different results (Pearson’s \\(r\\) is 0.74).\n\n\n\n\nCraig M. Bennett, Michael B. Miller, Abigail A. Baird. 2011. “Wolford.(2011). Neural Correlates of Interspecies Perspective Taking in the Post-Mortem Atlantic Salmon: An Argument for Multiple Comparisons Correction.”\n\n\nDomaszewska, Teresa, Lisa Scheuermann, Karin Hahnke, Hans Mollenkopf, Anca Dorhoi, Stefan HE Kaufmann, and January Weiner 3rd. 2017. “Concordant and Discordant Gene Expression Patterns in Mouse Strains Identify Best-Fit Animal Model for Human Tuberculosis.” Scientific Reports 7 (1): 12094.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>High Throughput Data Analysis</span>"
    ]
  },
  {
    "objectID": "reproducibility.html",
    "href": "reproducibility.html",
    "title": "4  Scientific Reproducibility",
    "section": "",
    "text": "4.1 Of mice, men and reproducibility\nIn 2013, a paper in PNAS made quite a splash. The authors compared transcriptomic responses to various human infommatory conditions (burns, trauma and inflammation) with the transcriptomic responses in the corresponding mouse models. The conclusion was in the title: Genomic responses in mouse models poorly mimic human inflammatory diseases.\nA bit over a year later, in the same journal, another paper was published. The title was identical… almost identical: instead of “poorly mimic”, the authors wrote “greatly mimic”. Their conclusion was, it turns out, that mouse models are actually pretty good at mimicking human inflammatory diseases.\nThe fact that two papers come up with diametrically opposed conclusions is, in itself, not that surprising. That is what science is about, after all. The first surprising thing about these two papers was that they were based on the same data. The second paper was a re-analysis of the data from the first paper – done in a different way, which focused on similarities. Among other things, they looked only at genes that were significantly up- or down-regulated in either species rather than looking at all noisy genes. Also, rather than using parametric statistics, they focused on non-parametric alternatives – possibly more suitable. After all, we are not so much interested whether a gene is up-regulated 1.7 times or 1.9 times, but rather whether it is up- or down-regulated.\nThe second surprising thing was when Teresa Domaszewska, my student at that time, tried to reproduce the findings of both papers. Our idea was that evolutionarily speaking, the lineages leading to mice and humans split about 90 million years ago. That means that we share a lot of common evolutionary history, and obviously there will be many conserved parts of the immune system common to both species. However, for the past 90 million years our species evolved separately, in different environments, with different evolutionary pressures, constraints and lifestyles. We believed that a third paper should be written, one where the word “partly” would replace “poorly” or “greatly”.\nHowever, it turned out that although the data were public, an exact reproduction of all the results was impossible. The authors of either of the papers did not provide the exact code (programs) that were used to generate the results. Lime many other scientists, we found that the published methods are pitifuly insufficient to reproduce the results.\nThis is, unfortunately, a very common situation in biomedical research.\nIn 2021, results of a large-scale effort to reproduce 193 experiments from 53 high impact papers in cancer biology were published. As in previous attempts in other fields of science, it turned out that more than half of the findings could not be reproduced, and even in cases where the results were reproducibile, the effect sizes were usually smaller than originally reported. However, that is not even the most worrying part. Clearly, a successful reproduction of a scientific finding does not necessarily mean that the original finding was correct, and a failure to reproduce is not necessarily a sure sign that the original finding was wrong, and after all half of the findings were reproducible.\nMuch more problematic was the fact that the original papers did not provide enough information to even attempt a reproduction:\nand:\nThis situation is, unfortunately, not unique to cancer biology and it saddens me to say that it is fairly common in bioinformatics as well. While usually the data necessary to reproduce the analyses included in a paper are obtainable, but the actual code (programs) used to generate the results are either not fully available or so poorly documented that reproducing the results while in theory possible in practice requires a large amount of work at best and is often next to impossible.\nI like to use the term accountable: if you have a scientific result, be that a figure in a manuscript, a table or a supplementary Excel file, you should be able to easily trace it back to the primary readouts – the data collected in the lab, output by the sequencing machine, entered in the Excel sheet by the medical professional examining the patient. Fortunately, the tools to achieve that are not only available, but they are widely used and pretty easy to learn.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Scientific Reproducibility</span>"
    ]
  },
  {
    "objectID": "reproducibility.html#of-mice-men-and-reproducibility",
    "href": "reproducibility.html#of-mice-men-and-reproducibility",
    "title": "4  Scientific Reproducibility",
    "section": "",
    "text": "Genomic responses in mouse models poorly mimic human inflammatory diseases (Seok et al. 2013)\n\n\n\nGenomic responses in mouse models greatly mimic human inflammatory diseases (Takao and Miyakawa 2015)\n\n\n\n\nOf mice, men and immunity: a case for evolutionary systems biology (Ernst and Carvunis 2018)\n\n\n\n\n\nInvestigating the replicability of preclinical cancer biology (Errington, Mathur, et al. 2021)\nChallenges for assessing replicability in preclinical cancer biology (Errington, Denis, et al. 2021)\n\n\n” Second, none of the 193 experiments were described in sufficient detail in the original paper to enable us to design protocols to repeat the experiments, so we had to seek clarifications from the original authors.” (Errington et al., 2021)\n\n\n\nChallenges for assessing replicability in preclinical cancer biology (Errington, Denis, et al. 2021)\n\n\n“Third, once experimental work started, 67% of the peer-reviewed protocols required modifications to complete the research and just 41% of those modifications could be implemented. (…) This experience draws attention to a basic and fundamental concern about replication – it is hard to assess whether reported findings are credible. (ibid.)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Scientific Reproducibility</span>"
    ]
  },
  {
    "objectID": "reproducibility.html#accountable-workflows-with-quarto-or-rmarkdown",
    "href": "reproducibility.html#accountable-workflows-with-quarto-or-rmarkdown",
    "title": "4  Scientific Reproducibility",
    "section": "4.2 Accountable workflows with Quarto or Rmarkdown",
    "text": "4.2 Accountable workflows with Quarto or Rmarkdown\nThe solution that I and many others advocate is to use what has been called by one of the pioneers of computer science, Donald Knuth, literate programming:\n\nLet us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do. “Literate Programming”, The Computer Journal 27 (1984), p. 97.\n\n\n\nLiterate programming (Knuth 1984)\nThe idea is that your whole analysis is a human readable document that contains both the text and the code (R scripts, Python programs, shell scripts…) that generate the presented results – figures, tables, Excel files… The document is then processed to produce the desired output format in a human readable form, and simultaneously the code is executed to actually generate the results.\n\n\n\n\n\nflowchart LR\n    A(Program + Text) --&gt;|knitr/Quarto| B(Text with analysis results)\n    B --&gt; C[LaTeX]\n    C --&gt; CC[PDF]\n    B --&gt; D[Word]\n    B --&gt; E[HTML]\n    B --&gt; F[Presentation]\n    B --&gt; G[Book]\n\n\n\n\n\n\nThis can be Rmarkdown, Quarto, Jupyter… the goal is that your code and your text are in one place, and the results of your calculations are entered automatically into the text.\nThe book that you are now reading is written using a system called Quarto, which is a modern version of Rmarkdown. If you are an R user, you probably use RStudio, which has excellent and native support for both Rmarkdown and Quarto. But Quarto is not limited to R; if you program with Python or Julia or even other languages, you can still use it in your workflow1.\n1 In fact, the diagram above was generated using a graph scripting language called Mermaid, which is also supported natively in Quarto.Here is how Quarto, Rmarkdown and similar systems work. Basically, you write your text as usual, including formatting in a way similar to how you emphasize text in emails (eg. stars like *this* result in italics like this). However, instead of writing up your results manually or copying and pasting figures from a separate program, you include the code that produces them directly in the text. For example, when I write that the p-value is equal to 0.05, I am writing literally this:\nFor example, when I write that the *p*-value is equal \nto `​r pval`, I am writing literally this:\nThe \\(p\\)-value above is not entered manually (as 0.05), but is the result of a statistical computation. If the data changes, if your analysis changes, the \\(p\\)-value above will automatically change as well.\nWhy is that a big deal? If you have ever written a manuscript or report, you probably know why: data change, analyses change, and each time a change happens, you have to go back to your manuscript and change every single occurence of the results, every figure, every table. Not only is that tedious, but it is also error-prone: you may forget to change one of the results, and then your manuscript is inconsistent with itself.\nBut that is not all. The most important part of is that your entire analysis, every operation on your data, every step, every statistical test or choice you have made is automatically documented in your code. Let me give you another example. I have prepared a little dataset, stored as a CSV file and available for download from github2.\n2 This is a tiny subset of the famous Iris dataset. You can find the whole dataset in R by typing iris into console.Here is the code that I include in my Quarto document to read the data and run a simple statistical test. I have opted to show the code below, but it does not have to be as visible as that in the final document.\n\n\nCode\n# the URL of the CSV file ON github\nfile &lt;- \n  \"https://github.com/bihealth/howtotalk-book/raw/refs/heads/main/scripts/iris.csv\"\n\n# read the data from the URL\ndata &lt;- read.csv(file)\n\n# run a t-test comparing sepal length between two species\n# and store the result in a variable\ntest_result &lt;- t.test(Sepal_Length ~ Species, data = data)\n\n\nI now can refer to the results of the statistical test in my text. For example, I can write that the difference in sepal length between the two species is statistically significant, with a p-value of 0.009, and that there were 40 samples in total. None of these numbers are entered manually; if the data change, the results will be updated when you run Quarto again.\nI can also include a figure just by writing the code that generates it. The figure is a so-called raincloud plot, which combines a boxplot, a violin plot and a scatter plot – all to demonstrate the distribution of data points.\n\n\nCode\nlibrary(ggplot2) # nice plotting library\nlibrary(ggrain)  # for raincloud plots\n\n# make a rain cloud\nggplot(data, aes(x=Species, y=Sepal_Length)) +\n  geom_rain(alpha = 0.5) +\n  theme_minimal() +\n  labs(y=\"Sepal length (cm)\", x=\"Species\")\n\n\n\n\n\n\n\n\nFigure 4.1: Raincloud plot showing sepal length in two species of iris\n\n\n\n\n\nYou can also take a look at the chapter High throughput data analysis in which you will find some figures and results generated from a real dataset. You can find the code corresponding to each figure and see precisely how I generated each result.\nQuarto and Rmarkdown, apart from the advantages mentioned above, have excellent support for citations and bibliographies, cross-references to figures and tables, and many other features that make writing scientific manuscripts a breeze. Unfortunately, the downside is that most of your collaborators and coauthors will not be familiar with these systems and you will still have to produce Word documents for them to work with.\n\n\n\n\nErnst, Peter B, and Anne-Ruxandra Carvunis. 2018. “Of Mice, Men and Immunity: A Case for Evolutionary Systems Biology.” Nature Immunology 19 (5): 421–25.\n\n\nErrington, Timothy M, Alexandria Denis, Nicole Perfito, Elizabeth Iorns, and Brian A Nosek. 2021. “Challenges for Assessing Replicability in Preclinical Cancer Biology.” Elife 10: e67995.\n\n\nErrington, Timothy M, Maya Mathur, Courtney K Soderberg, Alexandria Denis, Nicole Perfito, Elizabeth Iorns, and Brian A Nosek. 2021. “Investigating the Replicability of Preclinical Cancer Biology.” Elife 10: e71601.\n\n\nKnuth, Donald Ervin. 1984. “Literate Programming.” The Computer Journal 27 (2): 97–111.\n\n\nSeok, Junhee, H Shaw Warren, Alex G Cuenca, Michael N Mindrinos, Henry V Baker, Weihong Xu, Daniel R Richards, et al. 2013. “Genomic Responses in Mouse Models Poorly Mimic Human Inflammatory Diseases.” Proceedings of the National Academy of Sciences 110 (9): 3507–12.\n\n\nTakao, Keizo, and Tsuyoshi Miyakawa. 2015. “Genomic Responses in Mouse Models Greatly Mimic Human Inflammatory Diseases.” Proceedings of the National Academy of Sciences 112 (4): 1167–72.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Scientific Reproducibility</span>"
    ]
  },
  {
    "objectID": "metadata.html",
    "href": "metadata.html",
    "title": "5  Preparing Meta Data",
    "section": "",
    "text": "5.1 Excel and gene names\nIn 2016, some scientists noticed a curious little thing. In lists of genes provided as supplementary materials to published papers here and there gene names were replaced by dates. For example, a gene name like SEPT2 (which stands for septin 2) would be replaced by the date, “September 2nd”. Other identifiers, like the RIKEN identifier “2310009E13” were converted to numbers (in this case, \\(2.31 \\times 10^13\\)). When Mark Ziemann and his colleagues systematically went through the scientific literature they found that roughly 20% of the supplementary materials contained these errors.\nThe common denominator was Excel - the spreadsheet program. Excel automatically “corrects” data it imports, changing the data types to what it “thinks” is suitable; it found names like “SEPT1” to be abbreviations of dates (rather than gene names) and was converting them automatically to dates or numbers. Since this behavior is very hard to avoid and until recently could not be completely switched off, the HUGO Gene Nomenclature Committee reached the hard and controversial decision to rename the gene names. SEPT2 became SEPTIN2, for example, and MARCH1 (membrane associated ring-CH-type finger 1) is now MARCHF1.\nAnd that is not even what makes this story funny. Five years after the original publication, in 2021, Mark Ziemann with colleagues went out to check whether the situation was now better. I think that the title of their publication says it all: “Lesson not lerned”. Not only did the scientists not became more aware of the problems with their gene lists, but it turned out that Excel converts gene names based on month names in different languages:",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Preparing Meta Data</span>"
    ]
  },
  {
    "objectID": "metadata.html#excel-and-gene-names",
    "href": "metadata.html#excel-and-gene-names",
    "title": "5  Preparing Meta Data",
    "section": "",
    "text": "Gene name errors are widespread in the scientific literature (Ziemann, Eren, and El-Osta 2016)\n\n\n\nScientists rename human genes to stop Microsoft Excel from misreading them as dates (James Vincent 2020)\n\n\n\nGene name errors: Lessons not learned (Abeysooriya et al. 2021)\n\nIn a few cases, the human gene AGO2 was converted to Aug-02 (eg: PMC5537504 & PMC6244004), which may be due to Excel working in languages such as Italian, Spanish or Portugese. Similarly, the gene MEI1 was seen to be converted to May-01 (eg: PMC6065148 & PMC5877863) and could be due to the similarity with the Dutch (mei). In one article (PMC5908809), TAMM41 was apparently converted to “Jan-41” due to similarity with the month of January in Finnish (tammikuu).\n\n\n\n\n\nAbeysooriya, Mandhri, Megan Soria, Mary Sravya Kasu, and Mark Ziemann. 2021. “Gene Name Errors: Lessons Not Learned.” PLoS Computational Biology 17 (7): e1008984.\n\n\nJames Vincent. 2020. “Scientists Rename Human Genes to Stop Microsoft Excel from Misreading Them as Dates.” The Verge. August 6, 2020. https://www.theverge.com/2020/8/6/21355674/human-genes-rename-microsoft-excel-misreading-dates.\n\n\nZiemann, Mark, Yotam Eren, and Assam El-Osta. 2016. “Gene Name Errors Are Widespread in the Scientific Literature.” Genome Biology 17 (1): 177.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Preparing Meta Data</span>"
    ]
  },
  {
    "objectID": "visualizations.html",
    "href": "visualizations.html",
    "title": "6  Visualizing data",
    "section": "",
    "text": "6.1 Show the data!",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visualizing data</span>"
    ]
  },
  {
    "objectID": "visualizations.html#avoid-bar-charts",
    "href": "visualizations.html#avoid-bar-charts",
    "title": "6  Visualizing data",
    "section": "6.2 Avoid bar charts",
    "text": "6.2 Avoid bar charts",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visualizing data</span>"
    ]
  },
  {
    "objectID": "wheretogo.html",
    "href": "wheretogo.html",
    "title": "7  Where to go from here",
    "section": "",
    "text": "7.1 Using ChatGPT and other LLM (large language models) tools\nI hope that this little book was useful and that you would like to learn more on reproducible research and bioinformatics. The three things that I believe are most useful to almost any researcher are:\nEven if you are not going to use these tools yourself, gaining an insight into how they work will help you to communicate with your bioinformatician.\nStatistics. When teaching R to students I often say that learning how to code is much easier than learning statistics. Despite my formal training in statistics, I still find it hard to grasp some of the more advanced concepts and I know my limits. Unfortunately, the 101 statistic courses are by necessity less useful in real life than one would hope – using a t-test is simple enough, but in real world the applications are often much more complex. On top of it, many bioinformaticians or computational biologists have also a very limited understanding of statistics.\nCoding. Popular language choices include Python and R; Python is more widely spread among computer scientists, but has a slightly steeper learning curve. If you plan to code a lot, Python will be a good choice. R, on the other hand, is ideal for the casual data scientist. The R language is particularly well suited for both data science and statistics.\nQuarto / Rmarkdown. Rmarkdown is the R-only, older system, while Quarto is the modern variant which supports R, Python and other languages. Both are excellent tools for creating reproducible workflows. They are easy to learn (especially if you are using Rstudio) and allow a lot of flexibility in the produced output. In fact, this book that you are reading right now has been written in Quarto.\nThere are problems with using geneartive LLM tools like ChatGPT, Perplexity AI and others – problems which are of ethical, legal and practical nature. For example, the recent rush to use such tools results in a huge amount of energy consumption and thus CO2 emissions which could be avoided in a lot of cases.\nFurthermore, the AI not only isn’t all-knowing, it is also very hard to notice that you have hit its limits. Instead of acknowledging that it doesn’t know something, it will try to make up an answer which sounds plausible, but is actually wrong. This has been mockingly called “hallucinating” by some people. Therefore, if you have alternatives, you should prefer them over using AI tools.\nStill, there are cases where LLM tools can be very useful. If you are a beginner learning a programming language, especially one that is widely used and popular, like R or Python, LLM can be a surprisingly good tutor. It is patient, polite and can often explain things better than a human. This is the best application of LLM tools that I have found so far.\nWhat you should definitely not do is to use LLM tools to write your code for you unless you know exactly what you are doing and unless you understand exactly the code that the tool produces. If, say, you would like to produce a raincloud plot in R, ask the LLM how to do it. But then rather than just using the code directly, try to understand how it works – ask the LLM to explain the code to you until you are sure that you really know how it works. Only then try to code yourself, and when (inevitably) you encounter problems, get back to LLM and ask for help with the specific problem.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Where to go from here</span>"
    ]
  },
  {
    "objectID": "wheretogo.html#using-chatgpt-and-other-llm-large-language-models-tools",
    "href": "wheretogo.html#using-chatgpt-and-other-llm-large-language-models-tools",
    "title": "7  Where to go from here",
    "section": "",
    "text": "Do not use LLM tools to write your code\nUse LLM tools to\n\nlearn how to code\nunderstand\ndebug your code",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Where to go from here</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abeysooriya, Mandhri, Megan Soria, Mary Sravya Kasu, and Mark Ziemann.\n2021. “Gene Name Errors: Lessons Not Learned.” PLoS\nComputational Biology 17 (7): e1008984.\n\n\nCraig M. Bennett, Michael B. Miller, Abigail A. Baird. 2011.\n“Wolford.(2011). Neural Correlates of Interspecies Perspective\nTaking in the Post-Mortem Atlantic Salmon: An Argument for Multiple\nComparisons Correction.”\n\n\nDomaszewska, Teresa, Lisa Scheuermann, Karin Hahnke, Hans Mollenkopf,\nAnca Dorhoi, Stefan HE Kaufmann, and January Weiner 3rd. 2017.\n“Concordant and Discordant Gene Expression Patterns in Mouse\nStrains Identify Best-Fit Animal Model for Human Tuberculosis.”\nScientific Reports 7 (1): 12094.\n\n\nErnst, Peter B, and Anne-Ruxandra Carvunis. 2018. “Of Mice, Men\nand Immunity: A Case for Evolutionary Systems Biology.”\nNature Immunology 19 (5): 421–25.\n\n\nErrington, Timothy M, Alexandria Denis, Nicole Perfito, Elizabeth Iorns,\nand Brian A Nosek. 2021. “Challenges for Assessing Replicability\nin Preclinical Cancer Biology.” Elife 10: e67995.\n\n\nErrington, Timothy M, Maya Mathur, Courtney K Soderberg, Alexandria\nDenis, Nicole Perfito, Elizabeth Iorns, and Brian A Nosek. 2021.\n“Investigating the Replicability of Preclinical Cancer\nBiology.” Elife 10: e71601.\n\n\nJames Vincent. 2020. “Scientists Rename Human Genes to Stop\nMicrosoft Excel from Misreading Them as Dates.” The Verge. August\n6, 2020. https://www.theverge.com/2020/8/6/21355674/human-genes-rename-microsoft-excel-misreading-dates.\n\n\nKnuth, Donald Ervin. 1984. “Literate Programming.” The\nComputer Journal 27 (2): 97–111.\n\n\nNieuwenhuis, Sander, Birte U Forstmann, and Eric-Jan Wagenmakers. 2011.\n“Erroneous Analyses of Interactions in Neuroscience: A Problem of\nSignificance.” Nature Neuroscience 14 (9): 1105–7.\n\n\nSeok, Junhee, H Shaw Warren, Alex G Cuenca, Michael N Mindrinos, Henry V\nBaker, Weihong Xu, Daniel R Richards, et al. 2013. “Genomic\nResponses in Mouse Models Poorly Mimic Human Inflammatory\nDiseases.” Proceedings of the National Academy of\nSciences 110 (9): 3507–12.\n\n\nTakao, Keizo, and Tsuyoshi Miyakawa. 2015. “Genomic Responses in\nMouse Models Greatly Mimic Human Inflammatory Diseases.”\nProceedings of the National Academy of Sciences 112 (4):\n1167–72.\n\n\nWasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA\nStatement on p-Values: Context, Process, and Purpose.” The\nAmerican Statistician. Taylor & Francis.\n\n\nWeiner 3rd, January, Benedikt Obermayer, and Dieter Beule. 2022.\n“Venn Diagrams May Indicate Erroneous Statistical Reasoning in\nTranscriptomics.” Frontiers in Genetics 13: 818683.\n\n\nZiemann, Mark, Yotam Eren, and Assam El-Osta. 2016. “Gene Name\nErrors Are Widespread in the Scientific Literature.” Genome\nBiology 17 (1): 177.",
    "crumbs": [
      "Introduction",
      "References"
    ]
  }
]