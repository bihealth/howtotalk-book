[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "How to talk to your bioinformatician",
    "section": "",
    "text": "1 Introduction\nPlease note: this brochure is a work in progress, barely started at this stage. The statistics chapter is largely ready, but others are at various stages. However, there still is the presentation (https://bihealth.github.io/howtotalk) which is actually where my idea started.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#who-this-book-is-for",
    "href": "index.html#who-this-book-is-for",
    "title": "How to talk to your bioinformatician",
    "section": "1.1 Who this book is for",
    "text": "1.1 Who this book is for",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#who-am-i-to-tell-you-this-stuff",
    "href": "index.html#who-am-i-to-tell-you-this-stuff",
    "title": "How to talk to your bioinformatician",
    "section": "1.2 Who am I to tell you this stuff?",
    "text": "1.2 Who am I to tell you this stuff?",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#bioinformatics-statistics-and-computational-biology",
    "href": "index.html#bioinformatics-statistics-and-computational-biology",
    "title": "How to talk to your bioinformatician",
    "section": "1.3 Bioinformatics, statistics and computational biology",
    "text": "1.3 Bioinformatics, statistics and computational biology\nWe often refer to the whole general category of humans involved in biological data analysis using computers as “bioinformaticians”. However, strictly speaking, bioinformaticians in the proper sense of the word are those who develop algorithms, databases and software packages. Then, there are those who use these tools to actual data, and the more precise term for these people is “computational biologists”. Finally, there are those of us who walk around and criticize experimental designs and they are called statisticians.\n\n\n\n\n\n\n\nG\n\n\n\nstatistics\n\nStatistics\n\n\n\nhypothesis\\ntesting\n\nhypothesis\ntesting\n\n\n\nstatistics--hypothesis\\ntesting\n\n\n\n\npower analysis\n\npower analysis\n\n\n\nstatistics--power analysis\n\n\n\n\nlinear\\nmodeling\n\nlinear\nmodeling\n\n\n\nstatistics--linear\\nmodeling\n\n\n\n\nvisualizations\n\nvisualizations\n\n\n\nstatistics--visualizations\n\n\n\n\nBayesian\\nstatistics\n\nBayesian\nstatistics\n\n\n\nstatistics--Bayesian\\nstatistics\n\n\n\n\nbioinformatics\n\nBioinformatics\n\n\n\nalgorithms\n\nalgorithms\n\n\n\nbioinformatics--algorithms\n\n\n\n\nformalizations\n\nformalizations\n\n\n\nbioinformatics--formalizations\n\n\n\n\nstandards\n\nstandards\n\n\n\nbioinformatics--standards\n\n\n\n\ndatabases\n\ndatabases\n\n\n\nbioinformatics--databases\n\n\n\n\ncbio\n\nComputational\nbiology\n\n\n\ndata\\nscience\n\ndata\nscience\n\n\n\ncbio--data\\nscience\n\n\n\n\nfunctional\\ngenomics\n\nfunctional\ngenomics\n\n\n\ncbio--functional\\ngenomics\n\n\n\n\nusing tools\n\nusing tools\n\n\n\ncbio--using tools\n\n\n\n\nsequence\\nanalysis\n\nsequence\nanalysis\n\n\n\ncbio--sequence\\nanalysis\n\n\n\n\nComputers and science\n\nComputers\nand\nScience\n\n\n\nComputers and science--statistics\n\n\n\n\nComputers and science--bioinformatics\n\n\n\n\nComputers and science--cbio\n\n\n\n\nimplementation\n\nimplementation\n\n\n\nalgorithms--implementation\n\n\n\n\nmaintenance\n\nmaintenance\n\n\n\ndatabases--maintenance\n\n\n\n\ndata\\nstewardship\n\ndata\nstewardship\n\n\n\ndata\\nscience--data\\nstewardship\n\n\n\n\ngene set\\nenrichment\n\ngene set\nenrichment\n\n\n\nfunctional\\ngenomics--gene set\\nenrichment\n\n\n\n\nphylogenies\n\nphylogenies\n\n\n\nsequence\\nanalysis--phylogenies\n\n\n\n\np-values\n\np-values\n\n\n\nhypothesis\\ntesting--p-values\n\n\n\n\neffect sizes\n\neffect sizes\n\n\n\nhypothesis\\ntesting--effect sizes\n\n\n\n\nhierarchical\\nmodeling\n\nhierarchical\nmodeling\n\n\n\nlinear\\nmodeling--hierarchical\\nmodeling",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#what-bioinformaticians-like",
    "href": "index.html#what-bioinformaticians-like",
    "title": "How to talk to your bioinformatician",
    "section": "1.4 What bioinformaticians like",
    "text": "1.4 What bioinformaticians like",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "statistics.html",
    "href": "statistics.html",
    "title": "2  Statistics",
    "section": "",
    "text": "2.1 We are all Bayesians\nWhen talking about statistics to students, I like to show a simple coin trick. It isn’t really a trick at all, but for me realizing what it shows for the first time completely changed the way how I understand the whole field.\nImagine that I put a coin on a table, heads up. The first question I ask you is: what is the probability that the coin is heads up? What is the probability that the coin is tails up? You can clearly see that the coin is heads up, so you tell me that the first probability - heads up - is 100%, and the probability that it is tails up is 0%.\nI take the coin in my hand and as I am about to throw it in the air I ask you what is the probability that it lands heads up. Usually you will say 50%. That is an obvious answer. But now, I actually throw the coin, catch it and land it on my hand - but I still cover it with my hand, so none of us can see how the coin landed. I ask you again: what is the probability that the coin shows heads? Now you might start to get suspicious, but the answer is still pretty much obvious: 50%, 1:1.\nOK, but now the plot thickens. I raise my hand just a bit and take a peek on the coin. You still can’t see it, but I exclaim: “AHA!” - I know already how it landed. I ask my question again: what is the probability that it landed heads up? Maybe you tell me: it is still 50%. But wait, I have already seen whether it is heads up, right? For me the situation is the same as at the very beginning, when the coin was still on the table. So for me, it is either 0% - if it landed tails up - or 100%, if it showing heads?\nBut how can probability be relative? Can it be relative at all?\nThe answer is, maybe surprisingly: it can’t. At least not in the “regular”, frequentist statistics - the one that we use in most of our scientific papers. In fact, the very moment that the coin landed, the probability was 0 or 100%, whether we were aware of it or not.\nThat is why, in classical statistics, the p-values are not really to tell us any probability about our particular statistical test! If we were to compare two groups with a t-test, then either there are differences between the groups or there aren’t any - the coin has already landed, the probability is either 0% or 100%. What the p-values are meant to do is to keep is from making a mistake (specifically, a type I error) in the long run. However, whether we made a mistake or not is, in a particular case, 0% or 100%, we just don’t know which one. However, we can be reasonably confident that the percentage of such mistakes in our research will be less than 5% (or whichever p-value threshold we choose).\nOur intuition was therefore wrong… but was it, really?\nIt turns out – it was wrong, but only in the classic, frequentist statistics. However it was dead on right from the point of view of Bayesian statistics. In Bayesian statistics, probability is a measure of information. We start with a rough idea based on some prior knowledge - that the coins are generally fair and land 50% of the time showing heads. We do an experiment, and we update this knowledge by observing that it landed actually showing heads, and we increase the probability to 100%. Until we make the observation, we keep the prior - whether the coin has already landed or not, as long we don’t know how it landed, the probability is still 50%. Thus, our intuition is definitely bayesian.\nThis makes for an interesting conundrum: we normally use frequentist statistics, but we think in bayesian terms. This has profound and unfortunate consequences, because p-values are a part of the language of science. We use p-values and statistical hypothesis testing on a regular basis to communicate with other researchers, to tell them something about our results, but this language is unintuitive and poorly understood by many.\nIs the Bayesian statistics “more correct” than the frequentist approach? There is no easy answer to this question. The debates between frequentist statisticians and bayesians can actually get quite heated at times. However, whatever our views on frequentists statistics are, the majority of the scientific papers will still involve p-values of some sort of another, and that means that you need to understand what p-values are.\nTherefore, let us talk about what p-values are, what their drawbacks are and what we can do about it.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "statistics.html#we-are-all-bayesians",
    "href": "statistics.html#we-are-all-bayesians",
    "title": "2  Statistics",
    "section": "",
    "text": "One of the reasons that the “frequentist” hypothesis testing is so firmly established is that bayesian statistics requires way more computing resources, which only relatively recently became widely available to statisticians.\n\n\n\n\n\n\n\n\nFrequentist Statistics\nBayesian Statistics\n\n\n\n\n\n\n\n\n1. Probability is defined as the long-run frequency of events\n1. Probability represents a degree of belief or certainty about an event\n\n\n2. Parameters (like the “true value”) are fixed but unknown quantities.\n2. Parameters are treated as random variables with their own probability distributions.\n\n\n3. Asking about the probability of a hypothesis does not make sense\n3. Asking about the probability of a hypothesis is the main goal",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "statistics.html#how-to-understand-p-values",
    "href": "statistics.html#how-to-understand-p-values",
    "title": "2  Statistics",
    "section": "2.2 How to understand p-values",
    "text": "2.2 How to understand p-values\n\n2.2.1 What is a p-value?\nSay you want to run a statistical test - such as a t test - to decide whether two groups differ. You want to distinguish between two possibilities:\n\nThere are no differences between the groups\nThere are differences between the groups\n\nWe call the former situation \\(H_0\\) or the null hypothesis, and the latter (there are differences), the \\(H_1\\) or the alternative hypothesis.\nSay you run the test, and you got a p-value of 0.02. What does that mean? P-value is a probability (this is where the p in “p-value” comes from), but it is the probability of what, exactly?\nTake a moment and try to answer this question. Chances are, your answer is one of these:\n\nProbability that \\(H_0\\) is true (probability that there is no difference), given the data\nProbability that \\(H_1\\) is true (probability that there is a difference), given the data\nProbability that the data is random\nProbability that the observations are due to random chance\nProbability of getting the same data by random chance\n\nIf your answer is above, I have good and bad news. The bad news is that you are wrong, because the correct answer is not among the answers above. The good news is that you are in a good company, research shows that something like half of the scientists using p-values don’t really understand them. Oh wait, that means that a lot of scientists don’t understand p-values, that can’t be good - I guess I have only bad news for you, then.\nFor the purposes of this discussion, we can simplify a formal definition to the following:\n\n\n\n\n\n\nP-value is the probability of observing an effect at least as extreme (as the one we observed) given that \\(H_0\\) is true.\n\n\n\nIn other words, assuming that there are no differences, but keeping all our other assumptions about the data (for example, how it is distributed), we ask what is the probability of getting data that show the same or larger differences between the groups?\nLet us dive in it a bit deeper.\nWhen you run a t-test, you calculate the so-called t statistic. It is the measure of the difference between the groups that we observe. It takes into account the differences and how many data points we have. What we do in the t-test we are assuming that there are no differences between the group, and ask, how likely it is to get a t value equal or larger to the one that we got under this assumption.\n\n\n2.2.2 Statistical tests make assumptions\nAs you can see, this definition of p-value I have included here is closest to the last of the wrong answers I have written out before – “probability of getting the same data by random chance”. However, there is a subtle but extremely important difference between the formulations, and I don’t mean that one uses “same data” and the other uses “an effect at least as extreme”. What I am talking about is that we are not looking at random data. The effects that we calculate under \\(H_0\\) must fullfill a lot of assumptions of the statistical test.\nYou probably heard of one of this assumptions - the assumption of normality. Parametric tests such as the t-test assume that the data is (roughly) normally distributed. If your data is not normally distributed – and a lot of biological data is not normally distributed – then this assumption is violated, and the p-value is calculated based on a model that does not fit our data.\nHowever, we could successfully argue that, in fact, no real data is normally distributed. Consider such a trait as the height of a human being. This is one of the traits that are par excellence the examples of normally distributed data. However, an actually normal variable can take any value with a certain, non-zero probability. In other words, there should be a non-zero probability of having a human being with height, say, 10 kilometers, or maybe have a negative length of -20 cm. Which, of course, doesn’t make any sense.\n\n\nNormality testing. You can test the data for normality. However, you need to be cautious. First, without enough samples, you will never be able to reject the assumption of normality. And if you have a lot of samples, you will almost always reject the assumption of normality. In other words, whether you reject the assumption of normality or not depends to some extent on your number of samples. Some statisticians, myself included, think that testing for normality of the data is therefore a waste of time.\nHowever, we know that many parametric tests – such as t-test and ANOVA – ara actually fairly robust when it comes to the violation of normality. In other words, you can safely use them with data which is not really normally distributed. This is one of these things that make statisticians remember the famous quote of George E. P. Box: “All models are wrong, but some are useful”.\nHowever, there are also other assumptions, of which one of the most important ones is the assumption of independence.\n\n\n2.2.3 Assumption of independence\nThis actually happened: I have been comparing the transcriptomes of two groups of people, healthy controls and patients suffering from a disease. What I saw was a large number of genes related to sex chromosomes. When I got the full meta-data of the project, it turned out that while the patients were 90% male, almost all of the healthy controls were health care workers, primarily nurses – in other words, females.\nSuch a setup violates the assumption of independence: that each measurement is independent of other measurements. This makes any results of statistical hypothesis testing in this setup unreliable, whatever test you are using, whether it is parametric or not etc.\n\n\n\n\n\n\nAssumption of independence is really, really important.\n\n\n\nOne way of dealing with that is to use the sex of the individual as another coefficient in the model. This has the drawback of making the model more complex and decreasing your statistical power, but is otherwise a good way to proceed. Unless, of course, all the males are in one group, and all the females are in the other – in such a case there is no good way to study the data. You just wasted a lot of time and money.\nAnother way that the assumption of independence is typically violated is a batch effect. Some technologies are more prone to batch effects than others, and you should always attempt to preemptively plan your experiments in a way that minimizes potential batch effects. One such technique is randomization – you randomly assign your samples to batches, so batches and groups do not correlate.\nWhile batch effects can be removed to a certain degree – and there are specialized tools for that – this never works to a 100%, and even when it works, it will not help you with a botched experiment design, in which groups and batches coincide.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "statistics.html#problems-with-p-values",
    "href": "statistics.html#problems-with-p-values",
    "title": "2  Statistics",
    "section": "2.3 Problems with p-values",
    "text": "2.3 Problems with p-values\nP-values have all sorts of problems. Some of them are inherent and the result of p-value properties. Others are due to how we handle them.\n\n\nThe ASA statement on p-values: context, process, and purpose (Wasserstein and Lazar 2016)\n\n2.3.1 P-values have limited informative value\nWhat would you say if I told you that p-values are not informative about your specific experiment that you are considering? As we discussed, p-values are calculated assuming that there is no effect – for example, no difference between the groups. However, when we find that \\(p &lt; 0.05\\), we reject the \\(H_0\\). The p-value is then calculated based on an assumption that was rejected, that we decided is not real. In our world (in which there is a difference between the groups), the p-value does not describe any reality. It belongs to an alternate world, in which there was no difference between the groups.\nHowever, what p-values definitely do in a reasonable way is to control the false discovery rate. While it can be argued (as above) that p-values are not informative for the particular experiment we are considering, using p-values protects you from too many false positives in the long run. It works as follows: even assuming no effect (no differences between the groups), there is a certain probability of observing large differences once in a while. By carefully considering this possibility with p-values we are attempting to make sure that this rare events do not fool us more than once in 20 times (which corresponds to the p-value threshold of 0.05).\n\n\n\n\n\n\nP-values control they false positive rate (type I error rate). This is all they do.\n\n\n\nHowever, p-values are not XXX\n\n\n2.3.2 Absence of evidence is not evidence of absence\nThe very name “type I error” suggests that there is another, second type of error. Indeed there is: the type II error. While the type I error is getting a false positive – concluding a difference where there isn’t one – the other side of the coin is doing the opposite – concluding a lack of difference when there is, in fact, a difference. This is called a false negative.\nThe bad news is, p-values do absolutely nothing to help with controling the false negative rate. The even worse news is that these errors are really, really common. You have probably heard of “statistical power”: that is exactly the ability of a statistical test to avoid false negative errors. It turns out that even for most powerful tests in best possible settings this power rarely exceeds 80%: in other words, 20% of true differences are never discovered!\n\n\n\n\n\n\nLack of statistical significance does not “prove” that there is no difference. Instead of saying “there was no difference” say “we failed to detect a statistically significant difference”.\n\n\n\nThis has several practical consequences. One of the most important is that the fact that the difference between the groups was not statistically significant does not mean that there is no difference.\n\n\n2.3.3 Difference between statistically significant and non-significant is not significant\nAndrew Gelman coined the phrase “the difference between statistically significant and non-significant is not, itself, statistically significant”. What it means is this: if in one comparison you get a statistically significant difference, and in the other one you see that there is no significant difference, then you cannot conclude that the there is a difference between comparisons.\nOh my, I think I have utterly confused you now. I think, however, that the problem is important and worth looking into, because while it is a surprisingly common way of reasoning, it is utterly wrong and can lead you astray.\nConsider this. You have two strains of mice, a wild type (WT) and a knockout. Your hypothesis is that while in the WT the regulation of a cytokine production works correctly and the cytokine gets produced upon stimulation, the regulation is broken in the KO. You plan your experiment as follows: for each of the mouse strains, you have a control (Ctrl) and a treatment (Trt) group, and your response variable is the level of the cytokine you have measured.\nNow to evaluate your experiment, you compare Ctrl with Trt in the WT strain and also in the KO strain. You have two comparisons: in WT (\\(\\Delta\\)WT = TrtWT - CtrlWT) and in KO (\\(\\Delta\\)KO = TrtKO - CtrlKO). In both cases, you run a t-test. You find that the p-value from the first comparison (\\(\\Delta\\)WT) is significant (\\(p &lt; 0.05\\)), and the p-value from the second comparison (\\(\\Delta\\)KO) is not (\\(p &gt; 0.05\\)). Does that mean that you confirmed your hypothesis? Can you say that there is no difference in KO, but there is a difference in WT?\nThe short answer is: no, you can’t.\nYou were able to show that there is a significant difference in the WT strain, but you failed to show that there is a difference in the KO. And failing to show a difference is not the same as showing a lack of difference. This is erronous reasoning is widely spread – a while ago a team of researchers looked at bunch of publications and found that almost half of the scientists that could make this error, did it.\n\n\nErroneous analyses of interactions in neuroscience: a problem of significance (Nieuwenhuis, Forstmann, and Wagenmakers 2011).\nSo what should we do instead? In fact we have to run a single statistical test. We have the differences in KO (\\(\\Delta\\)KO), and the differences in the WT (\\(\\Delta\\)WT), and we need to test whether these differences differ, whether \\(\\Delta_{KO} \\neq \\Delta_{WT}\\). This difference of differences is called, in statistics, interaction. To put it in a slightly more formal way, you have a 2-factor experiment (with one factor being the mouse strain, KO or WT, and the other being the treatment or lack thereof) and what you need to test is the interaction between the two factors.\nThe bad news (I seem to be the bringer of bad news all throughout this chapter) is that this test for interaction is much weaker (in terms of statistical power) than a simple comparison between two groups. That means that in order to have the same chance to detect a real effect as in the case of a simple comparison, you have to use many more samples.\nBut wait, there’s more. More bad news to come.\n\n\n2.3.4 Spurious results can be very convincing\nOne of the most common applications of high throughput sequencing is RNA-Seq, in which we look at transcriptomes (so, expression of thousands of genes at once) in some samples. Quite frequently, one needs to use a two factor design, for example comparing control and treatment groups in either KO or WT strains. The correct way of analysing such an experiment is, as mentioned, testing for a significant interaction. However, quite frequently the analysis is done differently.\n\n\nVenn diagrams may indicate erroneous statistical reasoning in transcriptomics (Weiner 3rd, Obermayer, and Beule 2022).\nI have followed that different approach in a paper I wrote a few years ago. I used gene expression data from both COVID-19 patients and non-COVID-19 patients. In each of these two groups (C19 and non-C19) there were two groups of patients, which I will call for now G1 and G2 (I will tell you what they are in a few moments). The question was: is the difference between COVID-19 patients and non-COVID-19 patients influenced by whether the patients belong to G1 or G2? In other words, does it matter for COVID-19 if you are a G1 or a G2 patient? For example, is COVID-19 more severe in G1? Are immune responses different for G1 patients than for G2 patients?\n\nWhat you see on the top left on the figure above is a Venn diagram. It shows the number of genes which are significantly different between COVID-19 and non-COVID-19 patients in G1 and in G2, respectively. We can see that 431 genes are specifically different in G1 only, and 278 are specifically different in G2 only, while only 132 are common for the two groups. When you run gene set enrichment on these G1- or G2-specific genes, you will find different facets of the immune responses, like TLR-4 signalling in G1 and IL-7 mediated signalling in G2. It would be really easy to write a paper on how the groups G1 and G2 shape the immune responses to COVID-19. So not only we see that G1 and G2 differ, but we also see that these differences make biological sense.\nThe problem is that G1 and G2 are random samples from the same population. I have pulled these samples from one large group by random! There are absolutely no real differences between G1 and G2. And yet it was easy for me to find apperent differences, show them on a nice plot and even attribute certain specific biological processes to them. How did I do it?\nStart with the “specific” genes. What does it mean that a gene is in the group of 431 genes “specific” for G1? It means that it was significant in the comparison between COVID-19 and non-COVID-19 in G1, but was not significant in G2. But as I mentioned before, the fact that it was significant does not mean that it was not different in G2 or that it is specific for G1. Perhaps the p-value was 0.049 for G1 and 0.051 in G2, and the actual differences in G1 and G2 were similar.\nBut if all that we are seeing on the Venn diagram is white noise, how come we have these beautiful and meaningful gene set enrichment terms? The point is that not only genes not significant in G2 may, in fact, also differ between COVID-19 and non-COVID-19 patients, albeit we fail to detact this fact; they are also very likely to be also genes which are involved in the immune responses. If a gene is related to the IL-7 mediated signalling and is significantly different in G2, it will increase the chance of detecting this pathway in G2. However, the very same gene is likely to also be regulated in G1, but our failure to detect it - because of lack of statistical power or because of random noise – means that we can’t identify the IL-7 pathway in G1. That does not, however, mean that it is not there.\nThe correct analysis in this case is, as mentioned before, analysis of interactions. If done correctly, the analysis does not show any differences between G1 and G2.\n\n\n2.3.5 Statistically significant is not the same as significant\n\n\n2.3.6 We treat p-values with too much reverence\nchasing of 0.05, p-value hacking and cherry picking",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "htdata.html",
    "href": "htdata.html",
    "title": "3  High Throughput Data Analysis",
    "section": "",
    "text": "3.1 Arnolfini and Venice\nIn the previous chapter, we touched on analysis of high throughput (HT) data. Here, we will spare some more time to discuss some of the strong and weak points of HT data.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>High Throughput Data Analysis</span>"
    ]
  },
  {
    "objectID": "htdata.html#arnolfini-and-venice",
    "href": "htdata.html#arnolfini-and-venice",
    "title": "3  High Throughput Data Analysis",
    "section": "",
    "text": "There are more connections of HT data to impressionism. A famous software package for HT data analysis is called Seurat – from the name ofa famous French painter, Georges Seurat. Seurat painted using the so called pointillism technique, where small dots of color are applied in patterns to form an image. The corresponding analysis package is used, among others, to visualize single cell RNA-seq data, where each of the thousands of dots represents a single cell.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>High Throughput Data Analysis</span>"
    ]
  },
  {
    "objectID": "htdata.html#exploratory-vs-hypothesis-driven-research",
    "href": "htdata.html#exploratory-vs-hypothesis-driven-research",
    "title": "3  High Throughput Data Analysis",
    "section": "3.2 Exploratory vs Hypothesis-driven research",
    "text": "3.2 Exploratory vs Hypothesis-driven research",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>High Throughput Data Analysis</span>"
    ]
  },
  {
    "objectID": "htdata.html#statistics-and-ht",
    "href": "htdata.html#statistics-and-ht",
    "title": "3  High Throughput Data Analysis",
    "section": "3.3 Statistics and HT",
    "text": "3.3 Statistics and HT\n\n3.3.1 Correction for multiple testing / FDR\nIf you are reading this book, you probably have heard about adjusted p values or FDRs (false discovery rates). What are they and why do we need them?\nOver a decade ago, a group of scientists bought a dead fish on a fish market and brought it back to the lab, where they put it into a functional MRI (fMRI) machine. Here is what they did:\n\n\nNeural correlates of interspecies perspective taking in the post-mortem Atlantic salmon: An argument for proper multiple comparisons correction. (Craig M. Bennett 2011)\n\nThe task administered to the salmon involved completing an open-ended mentalizing task. The salmon was shown a series of photographs depicting human individuals in social situations with a specified emotional valence. The salmon was asked to determine what emotion the individual in the photo must have been experiencing.\n\nYes, you read this correctly. They put a dead fish into an fMRI and then asked it to look at photographs and to think. And then they measured the brain activity of the salmon and, quite surprisingly, they did find some apparent activity in the brain of the fish.\nThe problem was, you see, that at that time standard evaluation of fMRI involved performing a diabolical number of statistical tests, comparing voxels (3D pixels) between different conditions. And the standard way of determining which of them are significant was to use a simple (and lax) p-value threshold. The authors of the salmon study argued that this is not admissible, because you will always find some “significant” results, and argued for the use of multiple correction procedures1.\n1 The authors of the salmon poster gained a lot of deserved attention with their stunt, culminating with an IgNobel prize in neuroscience in 2012. You can still find their poster online.The problem of multiple testing is present throughout all of the high throughput data analysis techniques. Each time we do a statistical test with a predefined significance level (the notorious “\\(p &lt; 0.05\\)”), we are trying to limit the occurence of false positive results (type I errors). Using p-values and a threshold of 0.05 means that we are willing to accept, on average, 1 false positive result in 20 tests. However, if we run just one RNA-seq experiment with one comparison, we will be doing something like 10 to 20 thousands of statistical tests. In other words, at \\(p &lt; 0.05\\), we expect 500 to 1000 false positive results, even if there is no real difference between the conditions!\nThat is why we need to employ a procedure to control the false positive rate2. The widely used Benjamini-Hochberg procedure3 aims at controlling the false discovery rate (FDR). The principle is as follows: among all genes for which the FDR is below 0.05, we expect at most 5% false positives. Similarly, if we only consider genes with FDR below 0.01, only 1% of them are expected to be false positives. FDR is therefore not, strictly speaking, a p-value (even though we often call it “adjusted p-value” and use it as such).\n2 Actually, there are two general approaches: correcting the family wise error rate (FWER), which is the probability of making at least one type I error in a family of tests, and controlling the false discovery rate (FDR), which is the expected proportion of type I errors among the rejected hypotheses. Benjamini-Hochberg procedure is the latter, the Bonferroni correction is the former.3 Described in one of the most highly cited papers ever, with an order of magnitude more citations than the Watson and Crick paper on the structure of DNA.However, the side effect of multiple testing correction is that it severly reduces the statistical power of the analysis. In other words, many genes that are actually differentially expressed will have an FDR above the threshold of 0.05, and we will not be able to detect them. Consider this: if you have 500 genes at FDR &lt; 0.05, you expect that 25 of them are false positives - and therefore 475 are true positives, really differentially expressed. However, if at the same time you have 1500 genes at FDR &lt; 0.1, it means that within these 1500 genes there are 150 false positives (additional 125 to the 25 already counted at FDR &lt; 0.05), but also 1350 true positives - an additional 875 true positives with FDR between 0.05 and 0.1. These are true positives that we will miss at FDR &lt; 0.05!\nThe bottom line is that we control the type I error rate – the false positives – at the cost of increasing the type II error rate – the false negatives. This is a dirty compromise that we encounter in many places in statistics and machine learning4, but it is exacerbated in the context of HT data analysis.\n4 And even medicine: if you raise the specificity of a test (minimizing false positives) you will lower its sensitivity (increasing false negatives) and vice versa. Screening tests typically aim at a high sensitivity (to catch all potential patients), but the cost is a low specificity – and that is why you should not worry too much about a positive result of mammography or PSA test without further confirmation.\n\n\n\n\n\nMultiple testing correction is essential when analyzing HT data, but it severly reduces the statistical power of the analysis.\n\n\n\n\n\n3.3.2 To log or not to log\n\n\n3.3.3 The scourge of the absolute thresholds\nA question that often arises in the context of HT data analysis is: “so how many genes / proteins / metabolites are differentially expressed exactly?”. For me as a bioinformatician, this question is not easily answered, and I will try to explain why. In the following, I will talk mostly about (bulk) RNA-seq data, but what I say applies to all kinds of HT data as well. In this context, we often say “DEGs” (differentially expressed genes) to refer to genes that are expressed at different levels in two or more conditions.\nBut what is a DEG? How do you know a gene is a DEG? Quite often, we define the DEGs by the results of a statistical comparison we have made between two groups of samples. We define DEGs as those genes for which the FDR (false discovery rate) is below a certain threshold, e.g. 5% (\\(FDR &lt; 0.05\\)). Why 5%? This comes from the common practice in statistics, but that does not make it any less arbitrary. We could use 1% or 10%, or even 2.7%. And depending how we choose the threshold, the number of DEGs will be different.\nBut 5% has at least a certain meaning: it is an attempt to keep the fraction of false positives among the DEGs at a certain level, as explained in the previous section (“Correction for multiple testing”). However, we also add another arbitrary threshold: the absolute change of the log fold change. The problem is that the statistical power of the test to compare the results depends on the absolute expression level of a given gene. Simplifying the matter a bit, genes with high expression can be measured more confidently, so we can detect smaller changes in their expression. However, small changes in expression are often biologically not very interesting. Technically, we want to filter the DEGs not only based on statistical significance, but also on the effect size, i.e. the absolute change in expression.\nQuite often, we use a threshold of 2-fold change, at least a two-fold increase or two-fold decrease (which corresponds to an absolute log2 fold change of 1 or more). Again, this is completely arbitrary5, and many people use different thresholds, or none at all.\n5 Even more than the FDR, which is rooted in a hundred years of tolerating one mistake in 20.And of course, the actual number of DEGs will depend on the thresholds you have chosen. This is why, when asked to provide the number of DEGs, I sometimes ask back: “how many would you like me to find?”.\nHowever, given a particular definition in a paper or experiment, we could argue that the number of DEGs is well defined. This is true, but the problem starts when you actually try to pretend that the number of DEGs is informative and useful. Because my next question to you would be: what do you need the number of DEGs for? The truth is, it is often used as a proxy for the overall “amount of change”. For example, you might try to compare the number of DEGs between different experiments. This is how it quickly is reported in papers: “we found 1234 DEGs in treatment A compared to controls, but only 567 in treatment B, demonstrating that treatment A has a stronger effect on gene expression than treatment B”.\nUnfortunately, this may be completely misleading. First of all, the number of DEGs will strongly depend on the statistical power of the experiment. If in one comparison we are looking at a group of 10 samples vs 10 samples, and in the other we have only 3 vs 3 samples, even if the extent of regulation is the same in both comparisons, there will be fewer DEGs in the second comparison, simply because we have less statistical power to detect them.\nBut even if we keep the sample size the same, the number of DEGs will depend on the overall variability of the data, on small errors in the preparation of the samples, on outliers etc etc. Yes, it is true that if the overall extent of regulation is higher, the number of DEGs will likely to be higher as well, but the reverse doesn’t necessarily hold. In the following section, we will look at an example and alternative ways of getting an idea of the overall extent of regulation.\n\n\n\n\n\n\nAvoid using absolute thresholds if possible. They are much less meaningful that they seam. Use meaningful visualizations instead, e.g. volcano plots, MA plots or disco plots.\n\n\n\n\n\n3.3.4 DEGs and p-value trap: an example\nIn the code below, I will use data which can be found in the GEO database under accession number GSE156063. The expression data comes from patients who either suffered from COVID-19 or another respiratory disease (it is the same data set that I used to demonstrate the incorrect use of Venn diagrams in a previous chapter). The code to generate the results and figures below is included in this book for your pleasure.\n\n\nCode\nlibrary(DESeq2)\n# the results are loaded from the output of DESeq2-based analysis of the\n# data set using a pipeline called sea-snap (see\n# github.com/bihealth/seasnap-pipeline) and loaded with the R package\n# Rseasnap.\n# However, we will only use the raw counts and the covariate table here.\nlibrary(Rseasnap)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(cowplot)\n\n\n\n\nCode\npip &lt;- load_de_pipeline(\"DE_config.yaml\")\n\ncovar &lt;- get_covariates(pip)\ncnts &lt;- counts(get_deseq2(pip))\n\n\nFirst, a little demonstration. We will run the same comparison, between non-COVID viral infection and non-viral controls, twice: once with 5 randomly selected samples per group, and once with 5 additional samples per group, and we will compare the results.\n\n\nCode\n# let us write a little function so that we don't have to repeat the same\n# code twice. The function runs DESeq2 on a selected subset of samples\nrun_deseq2 &lt;- function(covar, cnts, sel,\n                       contrast = c(\"group\", \"other\", \"no\")) {\n  covar_sel &lt;- covar[sel, ]\n  cnts_sel &lt;- cnts[, sel]\n  ds &lt;- DESeqDataSetFromMatrix(\n    countData = cnts_sel,\n    colData = covar_sel,\n    design = ~ group\n  )\n  ds &lt;- DESeq(ds)\n  res &lt;- results(ds, contrast = contrast)\n  as.data.frame(res)\n}\n\ncovid &lt;- which(covar$group == \"SC2\")\nno_covid &lt;- which(covar$group == \"no\")\nother_resp &lt;- which(covar$group == \"other\")\n\n# set the random number seed so you can reproduce the results\nset.seed(12345) \n\nsel_5 &lt;- c(sample(no_covid, 5), sample(other_resp, 5))\nres_5 &lt;- run_deseq2(covar, cnts, sel_5)\n\n# add 5 more samples per group\nsel_10 &lt;- c(sel_5, sample(setdiff(no_covid, sel_5), 5), \n                   sample(setdiff(other_resp, sel_5), 5))\nres_10 &lt;- run_deseq2(covar, cnts, sel_10)\n\nsel_10_covid &lt;- c(sample(covid, 10), sample(other_resp, 10)) \nres_10_covid &lt;- run_deseq2(covar, cnts, sel_10_covid,\n                           contrast = c(\"group\", \"other\", \"SC2\"))\n\nres_m &lt;- merge(\n  res_5 %&gt;% \n    select(log2FC_5 = log2FoldChange, pval_5 = pvalue, padj_5 = padj) %&gt;% rownames_to_column(\"gene\"),\n  res_10 %&gt;% \n    select(log2FC_10 = log2FoldChange, pval_10 = pvalue, padj_10 = padj) %&gt;% rownames_to_column(\"gene\"),\n  by = \"gene\") |&gt; merge(\n  res_10_covid %&gt;% \n    select(log2FC_10_covid = log2FoldChange, pval_10_covid = pvalue, padj_10_covid = padj) %&gt;% rownames_to_column(\"gene\"),\n  by = \"gene\"\n)\n\n\nAt FDR &lt; 0.05 and with a 2-fold change threshold (\\(abs(log2FC) &gt; 1\\)), we find 1285 DEGs with 5 samples per group, but 2372 DEGs with 10 samples per group. This is quite a difference! I have seen papers in which much finer differences in the number of DEGs were used to argue that one condition has a stronger effect on gene expression than another. But are the results really that different? Let us take a look at the volcano plots, which show the log fold change on the x-axis and the negative log10 of the p-value on the y-axis6.\n6 The negative log10 of the p-value is used so that small p-values (which we are interested in) are more clearly distninguished.\n\nCode\ndf &lt;- rbind( res_5 |&gt; rownames_to_column(\"gene\") |&gt; mutate(set = \"5 samples\n            / group\"), res_10 |&gt; rownames_to_column(\"gene\") |&gt; mutate(set =\n            \"10 samples / group\")) |&gt; mutate(significant = ifelse(padj &lt;\n0.05 & abs(log2FoldChange) &gt; 1, \"yes\", \"no\"))\n\nggplot(df, aes(x = log2FoldChange, y = -log10(padj))) +\n  geom_point(aes(color = significant), alpha = 0.1) +\n  scale_color_manual(values = c(\"no\" = \"black\", \"yes\" = \"#880000\")) +\n  geom_hline(yintercept = -log10(0.05), linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = c(-1, 1), linetype = \"dashed\", color = \"blue\") +\n  facet_wrap(~set) +\n  xlab(\"log2 fold change\") +\n  ylab(\"-log10 adjusted p-value\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 3.1: Volcano plots for the comparison of non-COVID viral infection vs non-viral controls with 5 samples per group (left) and 10 samples per group (right).”\n\n\n\n\n\nAs you can see, there is hardly a big difference. Despite the difference in the absolute number of DEGs, the actual volcano plots are quite similar. We can further show that on a plot directly comparing the log2 fold changes. In addition, we will color the genes with a score that combines p-values and log2 fold changes, so called “disco” (discordance / concordance) score. Genes that show similar behavior in both comparisons come up red, while genes that go into opposite directions are blue.\n\n\nConcordant and discordant gene expression patterns in mouse strains identify best-fit animal model for human tuberculosis (Domaszewska et al. 2017)\nIn addition, to better appreciate the similarity of the results for 5 and 10 samples, on the other panel of the same figure I will throw another comparison into the mix: again with 10 samples per group, but this time COVID-19 patients vs non-viral controls.\n\n\nCode\n#|\n\n# cutoff threshold for disco score\ndthr &lt;- 20\n\n# calculate disco scores\nres_m &lt;- res_m |&gt;\n  mutate(disco_5_10 = log2FC_5 * log2FC_10 * abs(log10(pval_5) + log10(pval_10)),\n         disco_5_10_covid = log2FC_5 * log2FC_10_covid * abs(log10(pval_5) + log10(pval_10_covid))) |&gt;\n  mutate(disco_5_10 = ifelse(abs(disco_5_10) &gt; dthr, sign(disco_5_10) * dthr, disco_5_10),\n         disco_5_10_covid = ifelse(abs(disco_5_10_covid) &gt; dthr, sign(disco_5_10_covid) * dthr, disco_5_10_covid))\n\np1 &lt;- ggplot(res_m, aes(x = log2FC_5, y = log2FC_10, color = disco_5_10)) +\n  geom_point(alpha = 0.3) +\n  scale_color_gradient2(low = \"blue\", mid = \"grey\", high = \"red\", midpoint = 0) +\n  xlab(\"log2 fold change (5 samples / group)\") +\n  ylab(\"log2 fold change (10 samples / group)\") +\n  theme_minimal() +\n  ggtitle(\"Comparison of non-COVID viral infection vs non-viral controls\") +\n  theme(legend.position = \"none\") +\n  xlim(c(-6, 6)) + ylim(c(-6, 6)) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"black\")\n\np2 &lt;- ggplot(res_m, aes(x = log2FC_10, y = log2FC_10_covid, color = disco_5_10_covid)) +\n  geom_point(alpha = 0.3) +\n  scale_color_gradient2(low = \"blue\", mid = \"grey\", high = \"red\", midpoint = 0) +\n  xlab(\"log2 fold change (10 samples / group, non-COVID vs no)\") +\n  ylab(\"log2 fold change (10 samples / group, COVID vs no)\") +\n  theme_minimal() +\n  ggtitle(\"Comparison of COVID-19 vs non-viral controls\") +\n  theme(legend.position = \"none\") +\n  xlim(c(-6, 6)) + ylim(c(-6, 6)) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"black\")\n\nplot_grid(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\nFigure 3.2: Disco plots comparing log2 fold changes. Left: comparison of 5 samples per group vs 10 samples per group for non-COVID viral infection vs non-viral controls. Right: comparison of 10 samples per group for COVID viral infection vs non-viral controls.\n\n\n\n\n\nAs you can see, the results obtained for 5 and 10 samples per group are basically identical (Pearson’s \\(r\\) is 0.88). On the other hand, it is clear that the comparison of COVID-19 with non-viral disease, while similar, yields different results (Pearson’s \\(r\\) is 0.74).\n\n\n\n\nCraig M. Bennett, Michael B. Miller, Abigail A. Baird. 2011. “Wolford.(2011). Neural Correlates of Interspecies Perspective Taking in the Post-Mortem Atlantic Salmon: An Argument for Multiple Comparisons Correction.”\n\n\nDomaszewska, Teresa, Lisa Scheuermann, Karin Hahnke, Hans Mollenkopf, Anca Dorhoi, Stefan HE Kaufmann, and January Weiner 3rd. 2017. “Concordant and Discordant Gene Expression Patterns in Mouse Strains Identify Best-Fit Animal Model for Human Tuberculosis.” Scientific Reports 7 (1): 12094.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>High Throughput Data Analysis</span>"
    ]
  },
  {
    "objectID": "reproducibility.html",
    "href": "reproducibility.html",
    "title": "4  Scientific Reproducibility",
    "section": "",
    "text": "4.1 Reproducible workflows with Quarto or Rmarkdown\nflowchart LR\n    A(Program + Text) --&gt;|knitr/Quarto| B(Text with analysis results)\n    B --&gt; C[LaTeX]\n    C --&gt; CC[PDF]\n    B --&gt; D[Word]\n    B --&gt; E[HTML]\n    B --&gt; F[Presentation]\n    B --&gt; G[Book]\nThis can be Rmarkdown, Quarto, Jupyter… the goal is that your code and your text are in one place, and the results of your calculations are entered automatically into the text.\nIn systems such ar R markdown, you can put directly your analysis results in your text. For example, when I write that the \\(p\\)-value is equal to 0.05, I am writing this:\nThe \\(p\\)-value above is not entered manually (as 0.05), but is the result of a statistical computation. If the data changes, if your analysis changes, the \\(p\\)-value above will automatically change as well.\nWhy is that a big deal? If you have ever written a manuscript or report, you probably know why: data change, analyses change, and each time a change happens, you have to go back to your manuscript and change every single occurence of the results, every figure, every table. Not only is that tedious, but it is also error-prone: you may forget to change one of the results, and then your manuscript is inconsistent with itself.\nBut that is not all. The most important part of is that your entire analysis, every operation on your data, every step, every statistical test or choice you have made is automatically documented in your code. Take a look at the chapter High throughput data analysis in which you can find some figures and results generated from a real dataset. You can find the code corresponding to each figure and see precisely how I generated each result.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Scientific Reproducibility</span>"
    ]
  },
  {
    "objectID": "reproducibility.html#reproducible-workflows-with-quarto-or-rmarkdown",
    "href": "reproducibility.html#reproducible-workflows-with-quarto-or-rmarkdown",
    "title": "4  Scientific Reproducibility",
    "section": "",
    "text": "In systems such ar R markdown, you can put directly your\nanalysis results in your text. For example, when I write that the\n$p$-value is equal to `​r pval`, I am writing this:",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Scientific Reproducibility</span>"
    ]
  },
  {
    "objectID": "wheretogo.html",
    "href": "wheretogo.html",
    "title": "6  Where to go from here",
    "section": "",
    "text": "I hope that this little book was useful and that you would like to learn more on reproducible research and bioinformatics. The three things that I believe are most useful to almost any researcher are:\n\nStatistics\nCoding (likely R or Python)\nReproducible workflows with Quarto/Rmarkdown or Jupyter\n\nEven if you are not going to use these tools yourself, gaining an insight into how they work will help you to communicate with your bioinformatician.\nStatistics.\nCoding. Popular language choices include Python and R; Python is more widely spread among computer scientists, but has a slightly steeper learning curve. If you plan to code a lot, Python will be a good choice. R, on the other hand, is ideal for the casual data scientist. The R language is particularly well suited for both data science and statistics.\nQuarto / Rmarkdown. Rmarkdown is the R-only, older system, while Quarto is the modern variant which supports R, Python and other languages. Both are excellent tools for creating reproducible workflows. They are easy to learn (especially if you are using Rstudio) and allow a lot of flexibility in the produced output. In fact, this book that you are reading right now has been written in Quarto.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Where to go from here</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Craig M. Bennett, Michael B. Miller, Abigail A. Baird. 2011.\n“Wolford.(2011). Neural Correlates of Interspecies Perspective\nTaking in the Post-Mortem Atlantic Salmon: An Argument for Multiple\nComparisons Correction.”\n\n\nDomaszewska, Teresa, Lisa Scheuermann, Karin Hahnke, Hans Mollenkopf,\nAnca Dorhoi, Stefan HE Kaufmann, and January Weiner 3rd. 2017.\n“Concordant and Discordant Gene Expression Patterns in Mouse\nStrains Identify Best-Fit Animal Model for Human Tuberculosis.”\nScientific Reports 7 (1): 12094.\n\n\nNieuwenhuis, Sander, Birte U Forstmann, and Eric-Jan Wagenmakers. 2011.\n“Erroneous Analyses of Interactions in Neuroscience: A Problem of\nSignificance.” Nature Neuroscience 14 (9): 1105–7.\n\n\nWasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA\nStatement on p-Values: Context, Process, and Purpose.” The\nAmerican Statistician. Taylor & Francis.\n\n\nWeiner 3rd, January, Benedikt Obermayer, and Dieter Beule. 2022.\n“Venn Diagrams May Indicate Erroneous Statistical Reasoning in\nTranscriptomics.” Frontiers in Genetics 13: 818683.",
    "crumbs": [
      "Introduction",
      "References"
    ]
  }
]